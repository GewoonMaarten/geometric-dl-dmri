:py:mod:`autoencoder.models`
============================

.. py:module:: autoencoder.models


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   autoencoder.models.Encoder
   autoencoder.models.Decoder
   autoencoder.models.ConcreteAutoencoder
   autoencoder.models.BaseDecoder
   autoencoder.models.FCNDecoder
   autoencoder.models.SphericalDecoder



Functions
~~~~~~~~~

.. autoapisummary::

   autoencoder.models.init_weights_orthogonal



.. py:function:: init_weights_orthogonal(m)

   If Pytorch module is Linear then initialize the according to ``torch.nn.init.orthogonal``

   :param m: input module


.. py:class:: Encoder(input_size, output_size, max_temp = 10.0, min_temp = 0.1, reg_threshold = 3.0, reg_eps = 1e-10)

   Bases: :py:obj:`torch.nn.Module`

   Feature selection encoder
   Implemented according to "`Concrete Autoencoders for Differentiable Feature Selection and Reconstruction.`"
   :cite:p:`DBLP:journals/corr/abs-1901-09346`.

   :param input_size: size of the input layer. Should be the same as the `output_size` of the decoder.
   :param output_size: size of the latent layer. Should be the same as the `input_size` of the decoder.
   :param max_temp: maximum temperature for Gumble Softmax. Defaults to 10.0.
   :param min_temp: minimum temperature for Gumble Softmax. Defaults to 0.1.
   :param reg_threshold: regularization threshold. The encoder will be penalized when the sum of
                         probabilities for a selection neuron exceed this threshold. Defaults to 0.3.
   :param reg_eps: regularization epsilon. Minimum value for the clamped softmax function in
                   regularization term. Defaults to 1e-10.

   .. py:method:: latent_features(self)
      :property:


   .. py:method:: forward(self, x)

      Uses the trained encoder to make inferences.

      :param x: input data. Should be the same size as the encoder input.
      :type x: torch.Tensor

      :returns: encoder output of size `output_size`.
      :rtype: torch.Tensor


   .. py:method:: update_temp(self, current_epoch, max_epochs)


   .. py:method:: calc_mean_max(self)


   .. py:method:: regularization(self)

      Regularization term according to https://homes.esat.kuleuven.be/~abertran/reports/TS_JNE_2021.pdf. The sum of
      probabilities for a selection neuron is penalized if its larger than the threshold value. The returned value is
      summed with the loss function.



.. py:class:: Decoder(input_size, output_size, n_hidden_layers, negative_slope = 0.2)

   Bases: :py:obj:`torch.nn.Module`

   Standard decoder. It generates a network from `input_size` to `output_size`. The layers are generates as
   follows:
   ```python
   import numpy as np
   step_size = abs(output_size - input_size) // n_hidden_layers
   layer_sizes = np.arange(input_size, output_size, step_size)
   ```

   :param input_size: size of the latent layer. Should be the same as the `output_size` of the encoder.
   :param output_size: size of the output layer. Should be the same as `input_size` of the encoder.
   :param n_hidden_layers: number of hidden layers. If 0 then the input will be directly connected to the
   :param output.:
   :param negative_slope: negative slope for the Leaky ReLu activation layer. Defaults to 0.2.

   .. py:method:: forward(self, x)

      Uses the trained decoder to make inferences.

      :param x: input data. Should be the same size as the decoder input.
      :type x: torch.Tensor

      :returns: decoder output of size `output_size`.
      :rtype: torch.Tensor



.. py:class:: ConcreteAutoencoder(input_output_size = 1344, latent_size = 500, decoder_hidden_layers = 2, learning_rate = 0.001, max_temp = 10.0, min_temp = 0.1, reg_lambda = 0.0, reg_threshold = 1.0)

   Bases: :py:obj:`pytorch_lightning.LightningModule`

   Trains a concrete autoencoder
   Implemented according to "`Concrete Autoencoders for Differentiable Feature Selection and Reconstruction.`"
   :cite:p:`DBLP:journals/corr/abs-1901-09346`.

   :param input_output_size: size of the input and output layer.
   :param latent_size: size of the latent layer.
   :param decoder_hidden_layers: number of hidden layers for the decoder. Defaults to 2.
   :param learning_rate: learning rate for the optimizer. Defaults to 1e-3.
   :param max_temp: maximum temperature for Gumble Softmax. Defaults to 10.0.
   :param min_temp: minimum temperature for Gumble Softmax. Defaults to 0.1.
   :param reg_lambda: how much weight to apply to the regularization term. If the value is 0.0 then no regularization
                      will be applied. Defaults to 0.0.
   :param reg_threshold: regularization threshold. The encoder will be penalized when the sum of probabilities for a
                         selection neuron exceed this threshold. Defaults to 1.0.

   .. py:method:: forward(self, x)

      Uses the trained autoencoder to make inferences.

      :param x: input data. Should be the same size as encoder input.
      :type x: torch.Tensor

      :returns: (encoder output, decoder output)
      :rtype: tuple[torch.Tensor, torch.Tensor]


   .. py:method:: configure_optimizers(self)


   .. py:method:: training_step(self, batch, batch_idx)


   .. py:method:: validation_step(self, batch, batch_idx)


   .. py:method:: test_step(self, batch, batch_idx, dataloader_idx)


   .. py:method:: on_train_epoch_start(self)


   .. py:method:: on_epoch_end(self)


   .. py:method:: _shared_eval(self, batch, dataloader_idx, prefix)

      Calculate the loss for a batch.

      :param batch: batch data.
      :type batch: torch.Tensor
      :param batch_idx: batch id.
      :type batch_idx: int
      :param prefix: prefix for logging.
      :type prefix: str

      :returns: calculated loss.
      :rtype: torch.Tensor



.. py:class:: BaseDecoder(learning_rate, *args, **kwargs)

   Bases: :py:obj:`pytorch_lightning.LightningModule`

   Internal module used as a base for the :class:`.FCNDecoder` and the :class:`.SphericalDecoder`.

   :param learning_rate: Learning rate

   .. py:method:: configure_optimizers(self)


   .. py:method:: training_step(self, batch, batch_idx)


   .. py:method:: validation_step(self, batch, batch_idx)


   .. py:method:: test_step(self, batch, batch_idx, dataloader_idx)


   .. py:method:: _shared_eval(self, batch, batch_idx, prefix)

      Calculate the loss for a batch.

      :param batch: batch data.
      :type batch: torch.Tensor
      :param batch_idx: batch id.
      :type batch_idx: int
      :param prefix: prefix for logging.
      :type prefix: str

      :returns: calculated loss.
      :rtype: torch.Tensor



.. py:class:: FCNDecoder(input_size, output_size, hidden_layers = 2, learning_rate = 0.001)

   Bases: :py:obj:`BaseDecoder`

   Fully Connected Network decoder

   :param input_size: input size of the network
   :param output_size: output size of the network
   :param hidden_layers: number of hidden layers. Defaults to 2.
   :param learning_rate: learning rate. Defaults to 1e-3.

   .. py:method:: forward(self, x)



.. py:class:: SphericalDecoder(parameters_file_path, sh_degree, n_shells, learning_rate = 0.001)

   Bases: :py:obj:`BaseDecoder`

   Spherical decoder

   :param parameters_file_path: Path string of the parameters file
   :param sh_degree: Spherical Harmonics degree
   :param n_shells: Number of b-values
   :param learning_rate: Learning rate. Defaults to 1e-3.

   .. py:method:: forward(self, x)



