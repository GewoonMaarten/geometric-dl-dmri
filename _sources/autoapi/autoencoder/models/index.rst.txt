:orphan:

:py:mod:`autoencoder.models`
============================

.. py:module:: autoencoder.models


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   autoencoder.models.Encoder
   autoencoder.models.Decoder
   autoencoder.models.ConcreteAutoencoder
   autoencoder.models.FCNDecoder




.. py:class:: Encoder(input_size, output_size, max_temp = 10.0, min_temp = 0.1, reg_threshold = 3.0, reg_eps = 1e-10)

   Bases: :py:obj:`torch.nn.Module`

   Feature selection encoder. Implemented according to [_Concrete Autoencoders for Differentiable Feature Selection and Reconstruction_](https://arxiv.org/abs/1901.09346).

   :param input_size: size of the input layer. Should be the same as the `output_size` of the decoder.
   :type input_size: int
   :param output_size: size of the latent layer. Should be the same as the `input_size` of the decoder.
   :type output_size: int
   :param max_temp: maximum temperature for Gumble Softmax. Defaults to 10.0.
   :type max_temp: float, optional
   :param min_temp: minimum temperature for Gumble Softmax. Defaults to 0.1.
   :type min_temp: float, optional
   :param reg_threshold: regularization threshold. The encoder will be penalized when the sum of
   :type reg_threshold: float, optional
   :param probabilities for a selection neuron exceed this threshold. Defaults to 0.3.:
   :param reg_eps: regularization epsilon. Minimum value for the clamped softmax function in
   :type reg_eps: float, optional
   :param regularization term. Defaults to 1e-10.:

   .. py:method:: forward(self, x)

      Uses the trained encoder to make inferences.

      :param x: input data. Should be the same size as the encoder input.
      :type x: torch.Tensor

      :returns: encoder output of size `output_size`.
      :rtype: torch.Tensor


   .. py:method:: regularization(self)

      Regularization term according to https://homes.esat.kuleuven.be/~abertran/reports/TS_JNE_2021.pdf. The sum of
      probabilities for a selection neuron is penalized if its larger than the threshold value. The returned value is
      summed with the loss function.



.. py:class:: Decoder(input_size, output_size, n_hidden_layers, negative_slope = 0.2)

   Bases: :py:obj:`torch.nn.Module`

   Standard decoder. It generates a network from `input_size` to `output_size`. The layers are generates as
   follows:
   ```python
   import numpy as np
   step_size = abs(output_size - input_size) // n_hidden_layers
   layer_sizes = np.arange(input_size, output_size, step_size)
   ```

   :param input_size: size of the latent layer. Should be the same as the `output_size` of the encoder.
   :type input_size: int
   :param output_size: size of the output layer. Should be the same as `input_size` of the encoder.
   :type output_size: int
   :param n_hidden_layers: number of hidden layers. If 0 then the input will be directly connected to the
   :type n_hidden_layers: int
   :param output.:
   :param negative_slope: negative slope for the Leaky ReLu activation layer. Defaults to 0.2.
   :type negative_slope: float, optional

   .. py:method:: forward(self, x)

      Uses the trained decoder to make inferences.

      :param x: input data. Should be the same size as the decoder input.
      :type x: torch.Tensor

      :returns: decoder output of size `output_size`.
      :rtype: torch.Tensor



.. py:class:: ConcreteAutoencoder(input_output_size = 1344, latent_size = 500, decoder_hidden_layers = 2, learning_rate = 0.001, max_temp = 10.0, min_temp = 0.1, reg_lambda = 0.0, reg_threshold = 1.0)

   Bases: :py:obj:`pytorch_lightning.LightningModule`

   Trains a concrete autoencoder. Implemented according to [_Concrete Autoencoders for Differentiable Feature Selection and Reconstruction_](https://arxiv.org/abs/1901.09346).

   :param input_output_size: size of the input and output layer.
   :type input_output_size: int
   :param latent_size: size of the latent layer.
   :type latent_size: int
   :param decoder_hidden_layers: number of hidden layers for the decoder. Defaults to 2.
   :type decoder_hidden_layers: int, optional
   :param learning_rate: learning rate for the optimizer. Defaults to 1e-3.
   :type learning_rate: float, optional
   :param max_temp: maximum temperature for Gumble Softmax. Defaults to 10.0.
   :type max_temp: float, optional
   :param min_temp: minimum temperature for Gumble Softmax. Defaults to 0.1.
   :type min_temp: float, optional
   :param reg_lambda: how much weight to apply to the regularization term. If the value is 0.0 then no regularization will be applied. Defaults to 0.0.
   :type reg_lambda: float, optional
   :param reg_threshold: regularization threshold. The encoder will be penalized when the sum of probabilities for a selection neuron exceed this threshold. Defaults to 1.0.
   :type reg_threshold: float, optional

   .. py:method:: forward(self, x)

      Uses the trained autoencoder to make inferences.

      :param x: input data. Should be the same size as encoder input.
      :type x: torch.Tensor

      :returns: (encoder output, decoder output)
      :rtype: tuple[torch.Tensor, torch.Tensor]


   .. py:method:: _shared_eval(self, batch, dataloader_idx, prefix)

      Calculate the loss for a batch.

      :param batch: batch data.
      :type batch: torch.Tensor
      :param batch_idx: batch id.
      :type batch_idx: int
      :param prefix: prefix for logging.
      :type prefix: str

      :returns: calculated loss.
      :rtype: torch.Tensor



.. py:class:: FCNDecoder(input_size, output_size, hidden_layers = 2, learning_rate = 0.001)

   Bases: :py:obj:`BaseDecoder`

   Fully Connected Network decoder

   :param input_size: input size of the network
   :type input_size: int
   :param output_size: output size of the network
   :type output_size: int
   :param hidden_layers: number of hidden layers. Defaults to 2.
   :type hidden_layers: int, optional
   :param learning_rate: learning rate. Defaults to 1e-3.
   :type learning_rate: float, optional


