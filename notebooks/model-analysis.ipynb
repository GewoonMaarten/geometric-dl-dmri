{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284b898f",
   "metadata": {},
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from autoencoder.datasets import DiffusionMRIDataset, SphericalTransformer\n",
    "from autoencoder.logger import logger, set_log_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65300ee9-0e51-4969-b588-56b91a5cf440",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d833a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"ticks\", rc={\"figure.figsize\": (11.7 / 2, 8.27 / 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcd905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\"torch version %s\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f79d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available, else cpu\n",
    "has_cuda = torch.cuda.is_available()\n",
    "\n",
    "logger.info(\"Is the GPU available? %s\", has_cuda)\n",
    "device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "\n",
    "if has_cuda:\n",
    "    logger.info(\"Current device: %s\", torch.cuda.current_device())\n",
    "    logger.info(\"Device count: %s\", torch.cuda.device_count())\n",
    "    torch.cuda.set_device(0)\n",
    "    logger.info(\"Using device: %s\", torch.cuda.get_device_properties(device))\n",
    "else:\n",
    "    logger.warning(\"No GPU dectected! Training will be extremly slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a65c79",
   "metadata": {},
   "source": [
    "## Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_ip = \"localhost\"\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"http://{server_ip}:9000\"\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://{server_ip}:5000\")\n",
    "\n",
    "\n",
    "def get_mlflow_runs(experiment_id: int, filter_tags=None):\n",
    "    filter_tags = dict() if filter_tags is None else filter_tags\n",
    "    df_frames = list()\n",
    "    run_infos = mlflow.list_run_infos(str(experiment_id), run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY)\n",
    "    for run_info in run_infos:\n",
    "\n",
    "        run = mlflow.get_run(run_info.run_uuid)\n",
    "\n",
    "        for filter_key, filter_value in filter_tags.items():\n",
    "            if filter_key not in run.data.tags.keys() or run.data.tags[filter_key] != filter_value:\n",
    "                break\n",
    "        else:\n",
    "            logger.debug(\"Loading run info for: %s\", run_info.run_uuid)\n",
    "\n",
    "            metrics = {f\"metrics_{key}\": val for key, val in run.data.metrics.items()}\n",
    "            params = {f\"params_{key}\": val for key, val in run.data.params.items()}\n",
    "            tags = {f\"tags_{key}\": val for key, val in run.data.tags.items()}\n",
    "\n",
    "            features_dict = {**dict(run.info), **metrics, **params, **tags}\n",
    "\n",
    "            df_tmp = pd.DataFrame.from_records([features_dict])\n",
    "\n",
    "            df_tmp[\"end_time\"] = pd.to_datetime(df_tmp[\"end_time\"], unit=\"ms\")\n",
    "            df_tmp[\"start_time\"] = pd.to_datetime(df_tmp[\"start_time\"], unit=\"ms\")\n",
    "\n",
    "            df_frames.append(df_tmp)\n",
    "\n",
    "    df_runs = pd.concat(df_frames)\n",
    "    df_runs = df_runs.set_index(\"start_time\")\n",
    "    return df_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ecfad-e34c-44ac-8371-4f88c9dc0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fcn = get_mlflow_runs(3, dict(data=\"MUDI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8296e2f-c469-41de-b888-8112e161defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fourier_s2 = get_mlflow_runs(4, dict(data=\"MUDI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59475d",
   "metadata": {},
   "source": [
    "## Model evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98826f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"..\"\n",
    "IMAGES_PATH = Path(ROOT_PATH, \"images\")\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9431dbe",
   "metadata": {},
   "source": [
    "### Reconstruction loss (MSE) for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590fcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(subject, row, tissue=\"wb\"):\n",
    "    artifact_path = str(row.artifact_uri)\n",
    "\n",
    "    # load the data transformer if one was used during training.\n",
    "    transform = None\n",
    "    if \"params_transform\" in row.keys() and row.params_transform != \"None\":\n",
    "        transform_args = ast.literal_eval(row.params_transform)\n",
    "        if transform_args[\"class_path\"] == \"autoencoder.datasets.SphericalTransformer\":\n",
    "            transform = SphericalTransformer(**transform_args[\"init_args\"])\n",
    "\n",
    "    # Load the latent features. Replace the file: with / DOES NOT WORK IN WINDOWS\n",
    "    p = list(Path(artifact_path, \"latent_features.txt\").parts)\n",
    "    p[0] = \"/\"\n",
    "    features = np.loadtxt(Path(*p), dtype=np.int32)\n",
    "\n",
    "    # create the data set\n",
    "    data_set = DiffusionMRIDataset(\n",
    "        Path(\"..\", \"data\", \"prj_MUDI_parameters.hdf5\"),\n",
    "        Path(\"..\", \"data\", \"prj_MUDI_data.hdf5\"),\n",
    "        np.array([subject]),\n",
    "        tissue,\n",
    "        batch_size=256,\n",
    "        return_target=True,\n",
    "        include_parameters=features,\n",
    "        transform=transform,\n",
    "    )\n",
    "    data_gen = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=None,\n",
    "        batch_sampler=None,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # construct the model\n",
    "    model = mlflow.pytorch.load_model(str(Path(artifact_path, \"scripted_model\")))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # make the prediction\n",
    "    predictions = list()\n",
    "    targets = list()\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, batch in tqdm(enumerate(data_gen)):\n",
    "            if hasattr(data_gen.dataset, \"get_subject_id_by_batch_id\"):\n",
    "                subject_id = data_gen.dataset.get_subject_id_by_batch_id(batch_idx)\n",
    "                metadata = data_gen.dataset.get_metadata_by_subject_id(subject_id)\n",
    "            else:\n",
    "                raise Exception(\"Unknown dataset type. Could not get metadata\")\n",
    "\n",
    "            sample, target = batch[\"sample\"], batch[\"target\"]\n",
    "            if isinstance(batch[\"sample\"], dict):\n",
    "                for k in sample:\n",
    "                    sample[k] = sample[k].to(device)\n",
    "            else:\n",
    "                sample = sample.to(device)\n",
    "\n",
    "            prediction = model(sample)\n",
    "            prediction = (prediction.T / metadata[\"lstsq_coefficient\"] * metadata[\"max_data\"]).T\n",
    "            target = (target.T / metadata[\"lstsq_coefficient\"] * metadata[\"max_data\"]).T\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            targets.append(target)\n",
    "\n",
    "    prediction = torch.cat(predictions)\n",
    "    target = torch.cat(targets)\n",
    "\n",
    "    # return ground truth and prediction\n",
    "    return target.cpu(), prediction.cpu()\n",
    "\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"mean\").to(device)\n",
    "\n",
    "\n",
    "def calc_loss(row, subject, tissue):\n",
    "\n",
    "    target, prediction = predict(subject, row, tissue=tissue)\n",
    "    loss = mse_loss(prediction, target).to(\"cpu\")\n",
    "\n",
    "    losses = np.full(prediction.shape[1] + 3, np.nan)\n",
    "    for i in range(3, losses.shape[0]):\n",
    "        loss_raw = torch.nn.functional.mse_loss(prediction[:, i - 3], target[:, i - 3], reduction=\"none\")\n",
    "        losses[i] = (torch.sum(loss_raw) / loss_raw.shape[0]).to(\"cpu\")\n",
    "\n",
    "    losses[0] = np.mean(losses[3:])\n",
    "    losses[1] = np.median(losses[3:])\n",
    "    losses[2] = np.percentile(losses[3:], 85)\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def add_loss_metrics(df):\n",
    "    tissue_dfs = list()\n",
    "    for tissue in [\"gm\", \"wm\", \"csf\", \"wb\"]:\n",
    "        df_loss = df.apply(calc_loss, axis=1, args=[15, tissue], result_type=\"expand\")\n",
    "        column_names = [\"mean_loss\", \"median_loss\", \"85th_percentile_loss\"]\n",
    "        for i in range(3, len(df_loss.columns)):\n",
    "            column_names.append(str(i - 3))\n",
    "\n",
    "        df_loss.columns = column_names\n",
    "\n",
    "        x = pd.merge(df, df_loss[[\"mean_loss\", \"median_loss\", \"85th_percentile_loss\"]], on=\"start_time\")\n",
    "        x[\"tissue\"] = tissue\n",
    "\n",
    "        df_loss_tmp = df_loss.iloc[:, 3:].T\n",
    "        df_loss_tmp[\"feature\"] = df_loss_tmp.index\n",
    "\n",
    "        dfs = list()\n",
    "        for column in df_loss_tmp.columns[:-1]:\n",
    "            tmp = pd.DataFrame(df_loss_tmp)\n",
    "            tmp[\"start_time\"] = column\n",
    "            tmp = tmp[[column, \"start_time\", \"feature\"]]\n",
    "            tmp.columns = [\"loss\", \"start_time\", \"feature\"]\n",
    "            dfs.append(tmp)\n",
    "\n",
    "        df_loss = pd.concat(dfs)\n",
    "        df_loss = df_loss.set_index(\"start_time\")\n",
    "\n",
    "        tissue_dfs.append(pd.merge(x, df_loss, on=[\"start_time\"]))\n",
    "\n",
    "    return pd.concat(tissue_dfs)\n",
    "\n",
    "\n",
    "def clean_up_df(df):\n",
    "    df[\"tags_input_size\"] = pd.to_numeric(df[\"tags_input_size\"])\n",
    "    df = df.sort_values(by=[\"tags_input_size\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fourier_s2 = add_loss_metrics(df_runs_fourier_s2)\n",
    "df_runs_fourier_s2 = clean_up_df(df_runs_fourier_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3e782-569a-4e4a-a6f6-10cc345de0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fcn = add_loss_metrics(df_runs_fcn)\n",
    "df_runs_fcn = clean_up_df(df_runs_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a77fb-0fe6-46d0-8a0d-1dedf909b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_runs_fcn.drop_duplicates(subset=[\"artifact_uri\", \"tissue\"]).sort_values(by=[\"tags_run_group\", \"tissue\"])\n",
    "\n",
    "latent_size = 250\n",
    "# run_group = (\"FCN: random samples 1\",)\n",
    "run_group = (\"FCN: no regularisation\",)\n",
    "\n",
    "\n",
    "def to_table(row):\n",
    "    print(f\"{row.tissue}: ({row.mean_loss:.2e}, {row.median_loss:.2e}, {row['85th_percentile_loss']:.2e})\")\n",
    "\n",
    "\n",
    "x = x.loc[\n",
    "    (x.tags_input_size == latent_size) & (x.tags_run_group.isin(run_group)),\n",
    "    [\"tags_run_group\", \"tissue\", \"mean_loss\", \"median_loss\", \"85th_percentile_loss\"],\n",
    "]\n",
    "# .apply(to_table, axis=1)\n",
    "val = \"\"\n",
    "for tissue in [\"wb\", \"gm\", \"wm\", \"csf\"]:\n",
    "    row = x[x.tissue == tissue].iloc[0]\n",
    "    val += f\"  ({row.mean_loss:.4f}, {row.median_loss:.4f})\"\n",
    "    if tissue != \"csf\":\n",
    "        val += \" &\\n\"\n",
    "    else:\n",
    "        val += \" &\"\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78157df-2240-4f0d-92ed-c56af7671873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fcn = df_runs_fcn.sort_values(by=[\"tags_run_group\", \"tissue\"])\n",
    "df_runs_fourier_s2 = df_runs_fourier_s2.sort_values(by=[\"tags_run_group\", \"tissue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774566d-c8fb-41a2-8fa9-03e0c93fc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = pd.concat([df_runs_fcn, df_runs_fourier_s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175bcbcd-c1eb-43cb-b9f5-db48748e633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pointplot(\n",
    "    x=\"params_input_size\",\n",
    "    y=\"mean_loss\",\n",
    "    hue=\"params_hidden_layers\",\n",
    "    data=df_runs_fcn,\n",
    "    palette=\"rocket\",\n",
    "    hue_order=[\"2\", \"1\", \"3\"],\n",
    "    markers=[\"o\", \"s\", \"x\"],\n",
    "    dodge=True,\n",
    "    ci=None,\n",
    "    # linestyles=[\"-\", \"--\", \"-.\"],\n",
    ")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.set(\n",
    "    xlabel=\"Latent size\",\n",
    "    ylabel=\"MSE\",\n",
    ")\n",
    "sns.despine(trim=True, bottom=True)\n",
    "plt.legend(title=\"Num. decoder layers\", handles=handles, labels=[\"2\", \"3\", \"4\"])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(Path(IMAGES_PATH, \"FCN_decoder_sizes.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbabb9a",
   "metadata": {},
   "source": [
    "### Plot average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689d11c-9f7a-42c4-8421-5e1130cf3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.catplot(\n",
    "    x=\"tissue\",\n",
    "    y=\"loss\",\n",
    "    hue=\"tags_run_group\",\n",
    "    col=\"tags_input_size\",\n",
    "    col_wrap=2,\n",
    "    data=df_runs_fcn,\n",
    "    kind=\"box\",\n",
    "    palette=\"rocket\",\n",
    ")\n",
    "grid.set(yscale=\"log\")\n",
    "grid.despine(trim=True, bottom=True)\n",
    "grid.tight_layout()\n",
    "grid.set_titles(template=\"Latent size: {col_name}\")\n",
    "grid.set_ylabels(\"MSE\")\n",
    "grid.set_xlabels(\"Tissue type\")\n",
    "\n",
    "grid.legend.remove()\n",
    "handles, labels = grid.axes[1].get_legend_handles_labels()\n",
    "print(labels)\n",
    "grid.fig.legend(\n",
    "    handles,\n",
    "    [\"no regularisation\", \"regularisation\"],\n",
    "    loc=\"upper right\",\n",
    "    title=\"FCN Decoder:\",\n",
    ")\n",
    "grid.savefig(Path(IMAGES_PATH, \"HCP_FCN_loss_boxplot.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e39b5-f117-4a3f-aad1-c869dcc0fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.catplot(\n",
    "    x=\"tags_input_size\",\n",
    "    y=\"mean_loss\",\n",
    "    hue=\"tags_run_group\",\n",
    "    col=\"tissue\",\n",
    "    col_wrap=2,\n",
    "    data=df_runs_fcn,\n",
    "    kind=\"point\",\n",
    "    sharey=False,\n",
    "    palette=\"rocket\",\n",
    "    dodge=True,\n",
    "    markers=[\"o\", \"s\", \"x\", \"^\"],\n",
    "    ic=None,\n",
    ")\n",
    "# grid.set(yscale=\"log\")\n",
    "grid.despine(trim=True, bottom=True)\n",
    "grid.tight_layout()\n",
    "grid.set_titles(template=\"Tissue type: {col_name}\")\n",
    "grid.set_ylabels(\"MSE\")\n",
    "grid.set_xlabels(\"Latent size\")\n",
    "\n",
    "grid.legend.remove()\n",
    "handles, labels = grid.axes[1].get_legend_handles_labels()\n",
    "print(labels)\n",
    "grid.fig.legend(\n",
    "    handles,\n",
    "    [\"no regularisation\", \"regularisation\"],\n",
    "    loc=\"upper right\",\n",
    "    title=\"FCN Decoder:\",\n",
    ")\n",
    "grid.savefig(Path(IMAGES_PATH, \"FCN_loss_latent_size.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e36c02-6960-4054-a0e6-b59dd2e7f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.catplot(\n",
    "    x=\"tissue\",\n",
    "    y=\"loss\",\n",
    "    hue=\"tags_run_group\",\n",
    "    col=\"tags_input_size\",\n",
    "    col_wrap=2,\n",
    "    data=df_runs_fourier_s2,\n",
    "    kind=\"box\",\n",
    "    palette=\"rocket\",\n",
    "    sharey=False,\n",
    ")\n",
    "grid.set(yscale=\"log\")\n",
    "grid.despine(trim=True, bottom=True)\n",
    "grid.tight_layout()\n",
    "grid.set_titles(template=\"Latent size: {col_name}\")\n",
    "grid.set_ylabels(\"MSE\")\n",
    "grid.set_xlabels(\"Tissue type\")\n",
    "\n",
    "grid.legend.remove()\n",
    "handles, labels = grid.axes[1].get_legend_handles_labels()\n",
    "print(labels)\n",
    "grid.fig.legend(\n",
    "    handles,\n",
    "    [\"regularisation\", \"no regularisation\"],\n",
    "    loc=\"upper right\",\n",
    "    title=\"Fourier S2 Decoder:\",\n",
    ")\n",
    "grid.savefig(Path(IMAGES_PATH, \"HCP_Fourier_loss_boxplot.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5cd32-a3de-4e33-aa74-7308c8f28c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.catplot(\n",
    "    x=\"tags_input_size\",\n",
    "    y=\"mean_loss\",\n",
    "    hue=\"tags_run_group\",\n",
    "    col=\"tissue\",\n",
    "    col_wrap=2,\n",
    "    data=df_runs_fourier_s2,\n",
    "    kind=\"point\",\n",
    "    sharey=False,\n",
    "    palette=\"rocket\",\n",
    "    dodge=True,\n",
    "    markers=[\"o\", \"s\", \"x\", \"^\"],\n",
    ")\n",
    "# grid.set(yscale=\"log\")\n",
    "grid.despine(trim=True, bottom=True)\n",
    "grid.tight_layout()\n",
    "grid.set_titles(template=\"Tissue type: {col_name}\")\n",
    "grid.set_ylabels(\"MSE\")\n",
    "grid.set_xlabels(\"Latent size\")\n",
    "\n",
    "grid.legend.remove()\n",
    "handles, labels = grid.axes[1].get_legend_handles_labels()\n",
    "print(labels)\n",
    "grid.fig.legend(\n",
    "    handles,\n",
    "    [\"regularisation\", \"random samples\", \"no regularisation\"],\n",
    "    loc=\"upper right\",\n",
    "    title=\"Fourier S2 Decoder:\",\n",
    ")\n",
    "grid.savefig(Path(IMAGES_PATH, \"Fourier_loss_latent_size.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d210a-875d-4a2e-9ae6-1975e1c480e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_runs_fcn_f = df_runs_fcn[\n",
    "#     (df_runs_fcn.tags_run_group != \"FCN: random samples 0\")\n",
    "# ]\n",
    "# df_runs_fourier_s2_f = df_runs_fourier_s2[df_runs_fourier_s2.tags_run_group != \"Fourier S2: random samples\"]\n",
    "grid = sns.catplot(\n",
    "    x=\"tags_input_size\",\n",
    "    y=\"mean_loss\",\n",
    "    hue=\"tags_run_group\",\n",
    "    col=\"tissue\",\n",
    "    col_wrap=2,\n",
    "    data=pd.concat([df_runs_fcn, df_runs_fourier_s2]),\n",
    "    kind=\"point\",\n",
    "    sharey=False,\n",
    "    palette=\"rocket\",\n",
    "    dodge=True,\n",
    "    markers=[\"o\", \"s\", \"x\", \"^\"],\n",
    ")\n",
    "# grid.set(yscale=\"log\")\n",
    "grid.despine(trim=True, bottom=True)\n",
    "grid.tight_layout()\n",
    "grid.set_titles(template=\"Tissue type: {col_name}\")\n",
    "grid.set_ylabels(\"MSE\")\n",
    "grid.set_xlabels(\"Latent size\")\n",
    "\n",
    "grid.legend.remove()\n",
    "handles, labels = grid.axes[1].get_legend_handles_labels()\n",
    "print(labels)\n",
    "grid.fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"upper right\",\n",
    "    title=\"Decoder:\",\n",
    ")\n",
    "grid.savefig(Path(IMAGES_PATH, \"HCP_loss_latent_size.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea931e",
   "metadata": {},
   "source": [
    "### Plot feature occurence count\n",
    "\n",
    "Some features occure multiple times in a single model. Lets plot the top 20 most occuring features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc705ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feat_count(row):\n",
    "    model_uri = row.artifact_uri + \"/model\"\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    logits = model.encoder.logits\n",
    "    logits_size = logits.size()\n",
    "    features = torch.argmax(logits, len(logits_size) - 1)\n",
    "\n",
    "    counts = np.bincount(features)\n",
    "    counts_df = pd.DataFrame(counts, columns=[\"count\"])\n",
    "    return counts_df\n",
    "\n",
    "\n",
    "def get_feat_counts(data):\n",
    "    counts_dfs = []\n",
    "    if type(data) is pd.DataFrame:\n",
    "        for _, row in data.iterrows():\n",
    "            counts_df = get_feat_count(row)\n",
    "            counts_dfs.append(counts_df)\n",
    "    else:  # assume it is a Series\n",
    "        counts_df = get_feat_count(data)\n",
    "        counts_dfs.append(counts_df)\n",
    "    # sum all the bin counts\n",
    "    df_counts = pd.concat(counts_dfs).groupby(level=0).sum().reset_index()\n",
    "    return df_counts\n",
    "\n",
    "\n",
    "get_feat_count(df.iloc[0]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b76f1-613a-4ee9-b454-030bf1ba759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def feature_count(row):\n",
    "    print(\"lambda:\", row.params_lambda_reg)\n",
    "    print(\"val loss:\", row.metrics_val_loss)\n",
    "\n",
    "    model_uri = row.artifact_uri + \"/model\"\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    logits = model.encoder.logits\n",
    "    logits_size = logits.size()\n",
    "    features = torch.argmax(logits, len(logits_size) - 1).numpy()\n",
    "\n",
    "    eps = 1e-10\n",
    "    threshold = 3.0\n",
    "    selection = torch.clamp(F.softmax(logits, dim=0), eps, 1)\n",
    "    print(\"reg term:\", torch.sum(F.relu(torch.norm(selection, 1, dim=1) - threshold)))\n",
    "\n",
    "    return pd.DataFrame(features, columns=[\"feature\"])\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (40, 4)})\n",
    "sns.countplot(data=feature_count(df.iloc[2]), x=\"feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9b5f8-f785-4205-aa57-02dc90a874f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=feature_count(df.iloc[1]), x=\"feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e47d7a-3b0a-4edb-8c92-196a2463bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=feature_count(df.iloc[0]), x=\"feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a12cc2-547f-4fa3-b757-1b804f13dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674c3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
    "    \"\"\"Code from https://stackoverflow.com/a/56780852/6131485\"\"\"\n",
    "\n",
    "    def _show_on_single_plot(ax):\n",
    "        if h_v == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height()\n",
    "                value = int(p.get_height())\n",
    "                ax.text(_x, _y, value, ha=\"center\")\n",
    "        elif h_v == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() - float(space)\n",
    "                _y = p.get_y() + p.get_height() - 0.2\n",
    "                value = int(p.get_width())\n",
    "                ax.text(_x, _y, value, ha=\"right\", c=\"white\")\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97de29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_counts(df, top_size=10, ax=None, title=None):\n",
    "    df_counts = df.sort_values(by=\"count\", ascending=False)[:top_size]\n",
    "    plot = sns.barplot(\n",
    "        orient=\"h\",\n",
    "        x=\"count\",\n",
    "        y=\"index\",\n",
    "        data=df_counts,\n",
    "        order=df_counts[\"index\"].values,\n",
    "        palette=\"rocket\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    plot.set(xlabel=None, ylabel=None, title=title)\n",
    "    show_values_on_bars(plot, \"h\")\n",
    "    if ax is not None:\n",
    "        ax.grid(True, which=\"both\", ls=\"-\", c=\"lightgray\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(5 * 3, 5 * 5))\n",
    "# fig.suptitle(\"Volume counts for each model\", x=0.5, y=1)\n",
    "fig.text(0.5, -0.01, \"Count\", ha=\"center\")\n",
    "fig.text(-0.01, 0.5, \"Volume\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    row = df.iloc[i]\n",
    "    df_counts = get_feat_counts(row)\n",
    "    plot_counts(df_counts, ax=ax, title=f\"feat={row['n_features']} decoder={row['decoder']}\")\n",
    "\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "image_path = Path(IMAGES_PATH, \"feature_count.pdf\")\n",
    "plt.savefig(image_path, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 3, figsize=(5 * 3, 5 * 5))\n",
    "fig.suptitle(\"Feature counts for each model\", x=0.5, y=0.9)\n",
    "fig.text(0.5, 0.1, \"Count\", ha=\"center\")\n",
    "fig.text(0.07, 0.5, \"Feature\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    try:\n",
    "        row = df.iloc[15 + i]\n",
    "        df_counts = get_feat_counts(row)\n",
    "        plot_counts(df_counts, ax=ax, title=f\"feat={row['n_features']} decoder={row['decoder']}\")\n",
    "    except IndexError:\n",
    "        continue\n",
    "image_path = Path(IMAGES_PATH, \"feature_count_exclude.pdf\")\n",
    "plt.savefig(image_path, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20147787",
   "metadata": {},
   "source": [
    "### Interactive model plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image, masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5be6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    ColorBar,\n",
    "    ColumnDataSource,\n",
    "    LinearColorMapper,\n",
    "    LogColorMapper,\n",
    "    PreText,\n",
    "    RadioButtonGroup,\n",
    "    Select,\n",
    "    Slider,\n",
    "    Spinner,\n",
    ")\n",
    "from bokeh.plotting import figure\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478e001-64f5-4c59-af6d-3e374e8a7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fcn_f = df_runs_fcn.drop_duplicates(subset=[\"artifact_uri\"])\n",
    "df_runs_fourier_f = df_runs_fourier_s2.drop_duplicates(subset=[\"artifact_uri\"])\n",
    "\n",
    "\n",
    "def get_predict_mse(df, latent_size, run_group):\n",
    "    row = df[(df.tags_input_size == latent_size) & (df.tags_run_group == run_group)].iloc[0]\n",
    "    _, _, predict_mse = load_dmri(row, 15, return_mse=True)\n",
    "    return predict_mse\n",
    "\n",
    "\n",
    "def plot_imshow_rgb(axes, data, do_rot90: bool):\n",
    "    data = np.rot90(data) if do_rot90 else data\n",
    "    masked = np.dstack([data, ~(data[..., 1] == 0)])\n",
    "    axes.imshow(masked, interpolation=\"none\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_heatmap(axes, data, mask, do_rot90: bool, cbar_axes=None):\n",
    "    data = np.rot90(data) if do_rot90 else data\n",
    "    mask = np.rot90(mask) if do_rot90 else mask\n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        mask=mask == 0,\n",
    "        square=True,\n",
    "        xticklabels=False,\n",
    "        yticklabels=False,\n",
    "        ax=axes,\n",
    "        vmin=0,\n",
    "        vmax=0.06,\n",
    "        cbar=True if cbar_axes is not None else False,\n",
    "        cbar_ax=cbar_axes,\n",
    "        cbar_kws={\"orientation\": \"horizontal\"},\n",
    "    )\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(19, 15))\n",
    "gs0 = gridspec.GridSpec(2, 2, height_ratios=[4, 1], figure=fig)\n",
    "gs00 = gs0[0, 0].subgridspec(4, 3)\n",
    "\n",
    "latent_sizes = [500, 250, 100, 50]\n",
    "\n",
    "brain_mask = image.load_img(f\"/media/maarten/disk1/MUDI/cdmri0015/brain_mask.nii.gz\")\n",
    "brain_mask = np.asanyarray(brain_mask.dataobj)\n",
    "\n",
    "for i in range(gs00.nrows):\n",
    "    data = get_predict_mse(df_runs_fcn_f, str(latent_sizes[i]), \"FCN: no regularisation\")\n",
    "    s = data.shape\n",
    "\n",
    "    plot_heatmap(fig.add_subplot(gs00[i, 0]), data[s[0] // 2], brain_mask[s[0] // 2], True)\n",
    "    plot_heatmap(fig.add_subplot(gs00[i, 1]), data[:, s[1] // 2], brain_mask[:, s[1] // 2], True)\n",
    "    plot_heatmap(fig.add_subplot(gs00[i, 2]), data[:, :, 40], brain_mask[:, :, 40], False)\n",
    "\n",
    "\n",
    "gs01 = gs0[0, 1].subgridspec(4, 3)\n",
    "gs03 = gs0[1, 1].subgridspec(5, 1)\n",
    "\n",
    "for i in range(gs01.nrows):\n",
    "    data = get_predict_mse(df_runs_fourier_f, str(latent_sizes[i]), \"Fourier S2: no regularisation\")\n",
    "    s = data.shape\n",
    "\n",
    "    plot_heatmap(fig.add_subplot(gs01[i, 0]), data[s[0] // 2], brain_mask[s[0] // 2], True)\n",
    "    plot_heatmap(fig.add_subplot(gs01[i, 1]), data[:, s[1] // 2], brain_mask[:, s[1] // 2], True)\n",
    "    plot_heatmap(fig.add_subplot(gs01[i, 2]), data[:, :, 40], brain_mask[:, :, 40], False, fig.add_subplot(gs03[-1, 0]))\n",
    "\n",
    "gs02 = gs0[1, 0].subgridspec(1, 3)\n",
    "\n",
    "for i in range(gs02.nrows):\n",
    "    mask_3tt = image.get_data(image.load_img(\"/media/maarten/disk1/MUDI/cdmri0015/3tt.nii\"))\n",
    "    s = mask_3tt.shape\n",
    "\n",
    "    plot_imshow_rgb(fig.add_subplot(gs02[i, 0]), mask_3tt[s[0] // 2], True)\n",
    "    plot_imshow_rgb(fig.add_subplot(gs02[i, 1]), mask_3tt[:, s[1] // 2], True)\n",
    "    plot_imshow_rgb(fig.add_subplot(gs02[i, 2]), mask_3tt[:, :, 40], False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../images/cdb_mse_voxel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf827bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dmri(row, subject=15, return_mse=False):\n",
    "    target, prediction = predict(subject, row)\n",
    "\n",
    "    mask_path = f\"/media/maarten/disk1/MUDI/cdmri00{subject}/brain_mask.nii.gz\"\n",
    "\n",
    "    target_img = masking.unmask(np.transpose(target.numpy()), mask_path)\n",
    "    target_img = image.get_data(target_img)\n",
    "    prediction_img = masking.unmask(np.transpose(prediction.numpy()), mask_path)\n",
    "    prediction_img = image.get_data(prediction_img)\n",
    "\n",
    "    if return_mse:\n",
    "        prediction_mse = torch.nn.functional.mse_loss(target, prediction, reduction=\"none\")\n",
    "        prediction_mse_img = masking.unmask(np.transpose(prediction_mse.numpy()), mask_path)\n",
    "        prediction_mse_img = image.get_data(prediction_mse_img)\n",
    "        prediction_mse_img = np.sum(prediction_mse_img, axis=3) / prediction_mse_img.shape[3]\n",
    "\n",
    "        return target_img, prediction_img, prediction_mse_img\n",
    "    else:\n",
    "        return target_img, prediction_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ce6c9-5238-49d4-9580-6f3cade476d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fcn_f = df_runs_fcn.drop_duplicates(subset=[\"artifact_uri\"])\n",
    "df_runs_fcn_f = df_runs_fcn_f[df_runs_fcn_f.tags_input_size == \"500\"]\n",
    "\n",
    "# df_runs_fcn_f[df_runs_fcn_f.tags_input_size == 500]\n",
    "\n",
    "df_runs_fcn_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8f379-bc55-44cf-a887-14797a8b2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs_fourier_f = df_runs_fourier_s2.drop_duplicates(subset=[\"artifact_uri\"])\n",
    "df_runs_fourier_f = df_runs_fourier_f[df_runs_fourier_f.tags_input_size == \"500\"]\n",
    "df_runs_fourier_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354250e-6025-40c0-b0c3-a30f9d8915ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, prediction_img_fcn_mse = load_dmri(df_runs_fcn_f.iloc[-2], 15, return_mse=True)\n",
    "_, _, prediction_img_fourier_mse = load_dmri(df_runs_fourier_f.iloc[1], 15, return_mse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a748a-272b-4cef-8d68-dffa942b934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 22\n",
    "\n",
    "target = np.rot90(target_img[x, :, :, 0])\n",
    "predict_fcn = np.rot90(prediction_img_fcn[x, :, :, 0])\n",
    "predict_fourier = np.rot90(prediction_img_fourier[x, :, :, 0])\n",
    "\n",
    "# columns with all zeros\n",
    "idx = np.argwhere(np.all(target[..., :] == 0, axis=0))\n",
    "\n",
    "target = np.delete(target, idx, axis=1)\n",
    "predict_fcn = np.delete(predict_fcn, idx, axis=1)\n",
    "predict_fourier = np.delete(predict_fourier, idx, axis=1)\n",
    "\n",
    "fig = px.imshow(\n",
    "    np.array([target, predict_fcn, predict_fourier]),\n",
    "    facet_col=0,\n",
    "    template=\"seaborn\",\n",
    ")\n",
    "fig.layout.annotations[0][\"text\"] = \"Ground truth\"\n",
    "fig.layout.annotations[1][\"text\"] = \"Prediction: FCN Decoder<br>(latent size=500, no regularisation)\"\n",
    "fig.layout.annotations[2][\"text\"] = \"Prediction: Fourier S2 Decoder<br>(latent size=500, no regularisation)\"\n",
    "fig.update_layout(width=800, height=350, margin=dict(l=10, r=10, t=60, b=10))\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18323655-ac98-430c-949f-5445758c2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf823fd-0c36-4b78-a4f2-27e869356eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = prediction_img_fcn_mse.shape\n",
    "\n",
    "grid_kws = {\"height_ratios\": (0.45, 0.45, 0.05), \"hspace\": 0.1}\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10), gridspec_kw=grid_kws)\n",
    "# fig, (axes, cbar_ax) = plt.subplots(1, 3, figsize=(10,3), gridspec_kw=grid_kws)\n",
    "\n",
    "sns.heatmap(\n",
    "    np.rot90(prediction_img_fcn_mse[s[0] // 2]),\n",
    "    mask=np.rot90(brain_mask[s[0] // 2] == 0),\n",
    "    square=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=axes[0, 0],\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=0.06,\n",
    ")\n",
    "sns.heatmap(\n",
    "    np.rot90(prediction_img_fcn_mse[:, s[1] // 2]),\n",
    "    mask=np.rot90(brain_mask[:, s[1] // 2] == 0),\n",
    "    square=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=axes[0, 1],\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=0.06,\n",
    ").set(title=\"FCN decoder: no regularisation, latent size=500\")\n",
    "sns.heatmap(\n",
    "    prediction_img_fcn_mse[:, :, 40],\n",
    "    mask=brain_mask[:, :, 40] == 0,\n",
    "    square=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=axes[0, 2],\n",
    "    vmin=0,\n",
    "    vmax=0.06,\n",
    "    cbar=False,\n",
    "    # cbar_ax=cbar_ax[0],\n",
    "    # cbar_kws={\"orientation\": \"horizontal\"},\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    np.rot90(prediction_img_fourier_mse[s[0] // 2]),\n",
    "    mask=np.rot90(brain_mask[s[0] // 2] == 0),\n",
    "    square=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=axes[1, 0],\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=0.06,\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    np.rot90(prediction_img_fourier_mse[:, s[1] // 2]),\n",
    "    mask=np.rot90(brain_mask[:, s[1] // 2] == 0),\n",
    "    square=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=axes[1, 1],\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=0.06,\n",
    ").set(title=\"Fourier S2 decoder: no regularisation, latent size=500\")\n",
    "\n",
    "sns.heatmap(\n",
    "    prediction_img_fourier_mse[:, :, 40],\n",
    "    mask=brain_mask[:, :, 40] == 0,\n",
    "    square=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=axes[1, 2],\n",
    "    vmin=0,\n",
    "    vmax=0.06,\n",
    "    cbar_ax=axes[2, 1],\n",
    "    cbar_kws={\"orientation\": \"horizontal\"},\n",
    ")\n",
    "axes[2, 0].remove()\n",
    "axes[2, 2].remove()\n",
    "\n",
    "# sns.despine(left=True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.savefig(\"../images/mse_voxel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkapp(doc):\n",
    "    target_img, prediction_img = load_drmi(0)\n",
    "    source = ColumnDataSource(dict(target=[], prediction=[]))\n",
    "\n",
    "    x_max = target_img.shape[0] - 1\n",
    "    y_max = target_img.shape[1] - 1\n",
    "    z_max = target_img.shape[2] - 1\n",
    "    max_values = [x_max, y_max, z_max]\n",
    "\n",
    "    color_map = LogColorMapper(palette=\"Greys256\", low=0.01, high=255)\n",
    "\n",
    "    target_fig = figure(\n",
    "        title=\"Truth\",\n",
    "        tooltips=[(\"X\", \"$sx\"), (\"Y\", \"$sy\"), (\"Value\", \"@target\")],\n",
    "        toolbar_location=\"below\",\n",
    "        output_backend=\"webgl\",\n",
    "    )\n",
    "    target_fig.image(image=\"target\", source=source, x=0, y=0, dw=10, dh=10, color_mapper=color_map)\n",
    "\n",
    "    prediction_fig = figure(\n",
    "        title=\"Prediction\",\n",
    "        tooltips=[(\"X\", \"$sx\"), (\"Y\", \"$sy\"), (\"Value\", \"@prediction\")],\n",
    "        x_range=target_fig.x_range,\n",
    "        y_range=target_fig.y_range,\n",
    "        toolbar_location=\"below\",\n",
    "        output_backend=\"webgl\",\n",
    "    )\n",
    "    prediction_fig.image(\n",
    "        image=\"prediction\",\n",
    "        source=source,\n",
    "        x=0,\n",
    "        y=0,\n",
    "        dw=10,\n",
    "        dh=10,\n",
    "        color_mapper=color_map,\n",
    "    )\n",
    "    color_bar = ColorBar(color_mapper=color_map, label_standoff=12)\n",
    "    prediction_fig.add_layout(color_bar, \"right\")\n",
    "\n",
    "    options = list(\n",
    "        zip(\n",
    "            np.arange(len(df_runs)).astype(str),\n",
    "            list(df_runs.run_id),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_select = Select(\n",
    "        title=\"Model:\",\n",
    "        value=\"0\",\n",
    "        options=options,\n",
    "    )\n",
    "    slice_slider = Slider(start=0, end=x_max, value=0, step=1, title=\"Slice\")\n",
    "    feature_slider = Spinner(low=0, high=1343, value=0, step=1, title=\"Feature:\")\n",
    "    axis_radio = RadioButtonGroup(labels=[\"X\", \"Y\", \"Z\"], active=0)\n",
    "\n",
    "    def model_update():\n",
    "        target_img, prediction_img = load_drmi(int(model_select.value))\n",
    "        update()\n",
    "\n",
    "    def update():\n",
    "        feature_value = feature_slider.value\n",
    "        axis_value = axis_radio.active\n",
    "\n",
    "        slice_slider.end = max_values[axis_value]\n",
    "        if slice_slider.value > slice_slider.end:\n",
    "            slice_slider.value = slice_slider.end\n",
    "\n",
    "        slice_value = slice_slider.value\n",
    "\n",
    "        color_map.high = np.max(target_img)\n",
    "\n",
    "        if axis_value == 0:  # X\n",
    "            source.data = dict(\n",
    "                target=[target_img[slice_value, :, :, feature_value]],\n",
    "                prediction=[prediction_img[slice_value, :, :, feature_value]],\n",
    "            )\n",
    "        elif axis_value == 1:  # Y\n",
    "            source.data = dict(\n",
    "                target=[target_img[:, slice_value, :, feature_value]],\n",
    "                prediction=[prediction_img[:, slice_value, :, feature_value]],\n",
    "            )\n",
    "        elif axis_value == 2:  # Z\n",
    "            source.data = dict(\n",
    "                target=[target_img[:, :, slice_value, feature_value]],\n",
    "                prediction=[prediction_img[:, :, slice_value, feature_value]],\n",
    "            )\n",
    "\n",
    "    model_select.on_change(\"value\", lambda attr, old, new: model_update())\n",
    "    slice_slider.on_change(\"value\", lambda attr, old, new: update())\n",
    "    feature_slider.on_change(\"value\", lambda attr, old, new: update())\n",
    "    axis_radio.on_change(\"active\", lambda attr, old, new: update())\n",
    "\n",
    "    layout = row(\n",
    "        column(model_select, axis_radio, feature_slider, slice_slider),\n",
    "        target_fig,\n",
    "        prediction_fig,\n",
    "    )\n",
    "    doc.add_root(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ebc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"127.0.0.1:8888\"\n",
    "show(bkapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(bkapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bcdb2-a2ed-41c3-a0a7-3c0e69165b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('mudi': conda)",
   "language": "python",
   "name": "python3810jvsc74a57bd0a0d8aa0de02be0b131f97d6c1ce0bb8282df47d74804acd542e4e3d0f3fd66dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
