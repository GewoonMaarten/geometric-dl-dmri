{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing for MUDI data and Human Connectome Project (HCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from nilearn import image, plotting\n",
    "from nilearn.masking import apply_mask\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from autoencoder.logger import logger, set_log_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change these to your correct path**\n",
    "\n",
    "Make sure you have the following packages installed:\n",
    " - FSL: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUDI_PATH = Path(\"/media/maarten/disk1/MUDI/\")\n",
    "HCP_PATH = Path(\"/media/maarten/disk1/HCP/\")\n",
    "\n",
    "MUDI_OUTPUT_PATH = Path(\"/media/maarten/disk1/data_mudi.hdf5\")\n",
    "HCP_OUTPUT_PATH = Path(\"/media/maarten/disk1/data_hcp.hdf5\")\n",
    "\n",
    "FSL_INSTALL_PATH = \"/usr/local/fsl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FSLOUTPUTTYPE\"] = \"NIFTI_GZ\"\n",
    "os.environ[\"FSLDIR\"] = FSL_INSTALL_PATH\n",
    "os.environ[\"PATH\"] = f\"{FSL_INSTALL_PATH}/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Retrieving file paths\n",
    "\n",
    "We create a `dataclass` to store all the paths in. We can use these later to load the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MRISubjectData:\n",
    "    subject_name: str\n",
    "    subject_id: int\n",
    "    project: str  # Name of the dataset\n",
    "    output_file: Path\n",
    "\n",
    "    # paths\n",
    "    scheme_path: Path\n",
    "    root_path: Path  # path were all MRI data of the subject is located\n",
    "    dmri_path: Path  # relative to root_path\n",
    "    t1_path: Path  # relative to root_path\n",
    "    t2_path: Optional[Path] = None  # relative to root_path\n",
    "    brain_mask_path: Optional[Path] = None  # relative to root_path\n",
    "    fivett_mask_path: Optional[Path] = None\n",
    "\n",
    "    # normalization data\n",
    "    max_data: Optional[float] = 1.0\n",
    "    lstq_coefficient: Optional[float] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_mudi_subjects: list[MRISubjectData] = list()\n",
    "mri_hcp_subjects: list[MRISubjectData] = list()\n",
    "\n",
    "# MUDI subjects\n",
    "for i in range(1, 6):\n",
    "    subject = f\"cdmri001{i}\"\n",
    "\n",
    "    # check if we have a previously generated 5tt mask\n",
    "    fivett_mask_path = None\n",
    "    if Path(MUDI_PATH, subject, \"5tt.nii\").exists():\n",
    "        fivett_mask_path = Path(MUDI_PATH, subject, \"5tt.nii\")\n",
    "\n",
    "    mri_mudi_subjects.append(\n",
    "        MRISubjectData(\n",
    "            subject,\n",
    "            10 + i,\n",
    "            \"MUDI\",\n",
    "            MUDI_OUTPUT_PATH,\n",
    "            Path(MUDI_PATH, \"parameters_new.txt\"),\n",
    "            Path(MUDI_PATH, subject),\n",
    "            Path(\"MB_Re_t_moco_registered_applytopup.nii.gz\"),\n",
    "            Path(\"additional/anatomical.nii.gz\"),\n",
    "            brain_mask_path=Path(\"brain_mask.nii.gz\"),\n",
    "            fivett_mask_path=fivett_mask_path,\n",
    "        )\n",
    "    )\n",
    "\n",
    "logger.info(f\"found {len(mri_mudi_subjects)} MUDI subjects\")\n",
    "\n",
    "# HCP subjects\n",
    "for i in range(1, 36):\n",
    "    if i == 20:\n",
    "        continue  # skip subject 20, it has fewer volumes than the rest\n",
    "\n",
    "    subject = f\"mgh_10{i:02d}\"\n",
    "    # search for the mri files\n",
    "    dmri_path = list(HCP_PATH.glob(f\"**/*{subject}_MR_ep2d_Diffusion_Gradwarped,_Eddy_Current_Corrected_Br_*.nii\"))[0]\n",
    "    t1_path = list(HCP_PATH.glob(f\"**/*{subject}_MR_MPRAGE*.nii\"))[0]\n",
    "\n",
    "    # check if we have a previously generated brain mask\n",
    "    brain_mask_path = None\n",
    "    if Path(HCP_PATH, subject, \"brain_mask.nii\").exists():\n",
    "        brain_mask_path = Path(HCP_PATH, subject, \"brain_mask.nii\")\n",
    "\n",
    "    # check if we have a previously generated 5tt mask\n",
    "    fivett_mask_path = None\n",
    "    if Path(HCP_PATH, subject, \"5tt.nii\").exists():\n",
    "        fivett_mask_path = Path(HCP_PATH, subject, \"5tt.nii\")\n",
    "\n",
    "    # load and combine all MRI parameters\n",
    "    scheme_path = Path(HCP_PATH, subject, f\"{subject}_scheme.txt\")\n",
    "    if not scheme_path.exists():\n",
    "        bvals_path = Path(HCP_PATH, subject, f\"{subject}_bvals_ep2d_eddy.txt\")\n",
    "        bvecs_path = Path(HCP_PATH, subject, f\"{subject}_bvecs_moco_norm_ep2d_eddy.txt\")\n",
    "        bvals = np.loadtxt(bvals_path)[:, np.newaxis]\n",
    "        bvecs = np.loadtxt(bvecs_path)\n",
    "        scheme = np.c_[bvecs, bvals]\n",
    "        np.savetxt(scheme_path, scheme, fmt=\"%.6f %.6f %.6f %d\")\n",
    "\n",
    "    mri_hcp_subjects.append(\n",
    "        MRISubjectData(\n",
    "            subject,\n",
    "            i,\n",
    "            \"HCP\",\n",
    "            HCP_OUTPUT_PATH,\n",
    "            scheme_path,\n",
    "            Path(HCP_PATH, subject),\n",
    "            dmri_path.relative_to(Path(HCP_PATH, subject)),\n",
    "            t1_path.relative_to(Path(HCP_PATH, subject)),\n",
    "            brain_mask_path=brain_mask_path,\n",
    "            fivett_mask_path=fivett_mask_path,\n",
    "        )\n",
    "    )\n",
    "\n",
    "logger.info(f\"found {len(mri_hcp_subjects)} HCP subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating and applying a brain mask\n",
    "Create a brain mask from dwi image if none was provided. Based on the script bet from FSL toolbox. Docs: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET/UserGuide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brain_mask(subject: MRISubjectData):\n",
    "    \"\"\"\"\"\"\n",
    "    logger.info(\"starting brain mask generation\")\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"bet\",\n",
    "            Path(subject.root_path, subject.dmri_path),\n",
    "            Path(subject.root_path, \"brain.nii\"),\n",
    "            \"-m\",\n",
    "            \"-n\",\n",
    "            \"-f\",\n",
    "            \"0.5\",\n",
    "        ]\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        subject.brain_mask_path = Path(subject.root_path, \"brain_mask.nii.gz\")\n",
    "    logger.info(\"finished brain mask generation, return code: %d\", result.returncode)\n",
    "    return result.returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the brain mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scan(subject: MRISubjectData) -> np.ndarray:\n",
    "    dmri_path = Path(subject.root_path, subject.dmri_path)\n",
    "    brain_mask_path = None\n",
    "    if subject.brain_mask_path:\n",
    "        brain_mask_path = Path(subject.root_path, subject.brain_mask_path)\n",
    "    else:\n",
    "        result_code = generate_brain_mask(subject)\n",
    "        if result_code != 0:\n",
    "            logger.error(\"could not generate brain mask\")\n",
    "            return\n",
    "        brain_mask_path = Path(subject.root_path, subject.brain_mask_path)\n",
    "    return np.transpose(apply_mask(imgs=str(dmri_path), mask_img=str(brain_mask_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dmri_data(scan_data: np.ndarray, subject: MRISubjectData):\n",
    "    logger.info(\"storing data for subject %s\", subject.subject_name)\n",
    "\n",
    "    index = np.full(shape=scan_data.shape[0], fill_value=subject.subject_id, dtype=np.int8)\n",
    "    unnormalize_data = np.full(\n",
    "        shape=(scan_data.shape[0], 2),\n",
    "        fill_value=(subject.lstsq_coefficient, subject.max_data),\n",
    "        dtype=np.float64,\n",
    "    )\n",
    "\n",
    "    with h5py.File(subject.output_file, \"a\") as file:\n",
    "        if \"data\" not in file.keys():\n",
    "            file.create_dataset(\"data\", data=scan_data, chunks=True, maxshape=(None, scan_data.shape[1]))\n",
    "            file.create_dataset(\"index\", data=index, chunks=True, maxshape=(None,))\n",
    "            file.create_dataset(\n",
    "                \"unnormalize_data\", data=unnormalize_data, chunks=True, maxshape=(None, unnormalize_data.shape[1])\n",
    "            )\n",
    "        else:\n",
    "            file[\"data\"].resize(file[\"data\"].shape[0] + scan_data.shape[0], axis=0)\n",
    "            file[\"data\"][-scan_data.shape[0] :] = scan_data\n",
    "\n",
    "            file[\"index\"].resize(file[\"index\"].shape[0] + index.shape[0], axis=0)\n",
    "            file[\"index\"][-index.shape[0] :] = index\n",
    "\n",
    "            file[\"unnormalize_data\"].resize(file[\"unnormalize_data\"].shape[0] + unnormalize_data.shape[0], axis=0)\n",
    "            file[\"unnormalize_data\"][-unnormalize_data.shape[0] :] = unnormalize_data\n",
    "\n",
    "\n",
    "def normalize_data(subjects: list[MRISubjectData]):\n",
    "    scheme = np.loadtxt(subjects[0].scheme_path)\n",
    "    mask_b_0 = scheme[:, 3] == 0.0\n",
    "    subject_0 = subjects[0]\n",
    "\n",
    "    logger.info(\"normalizing subject %s (subject 0)\", subject_0.subject_name)\n",
    "\n",
    "    scan_data_subject_0 = load_scan(subject_0)\n",
    "\n",
    "    max_data = np.percentile(scan_data_subject_0, 95)\n",
    "    logger.info(\"95th percentile: %f\", max_data)\n",
    "    subject_0.max_data = max_data\n",
    "    subject_0.lstsq_coefficient = 1.0\n",
    "\n",
    "    scan_data_median_subject_0 = np.median(scan_data_subject_0[:, mask_b_0], axis=0)\n",
    "    scan_data_normalized = scan_data_subject_0.astype(\"float32\") * subject_0.lstsq_coefficient / subject_0.max_data\n",
    "\n",
    "    store_dmri_data(scan_data_normalized, subject_0)\n",
    "\n",
    "    for subject in subjects[1:]:\n",
    "        logger.info(\"normalizing subject %s\", subject.subject_name)\n",
    "        scan_data = load_scan(subject)\n",
    "        median_scan = np.median(scan_data[:, mask_b_0], axis=0)\n",
    "\n",
    "        scan_lstsq_coef, _, _, _ = np.linalg.lstsq(median_scan[:, np.newaxis], scan_data_median_subject_0, rcond=-1)\n",
    "        logger.info(\"lstsq coefficient for subject %s: %f\", subject.subject_name, scan_lstsq_coef[0])\n",
    "\n",
    "        subject.lstsq_coefficient = scan_lstsq_coef[0]\n",
    "        subject.max_data = max_data\n",
    "\n",
    "        scan_data_normalized = scan_data.astype(\"float32\") * subject.lstsq_coefficient / subject.max_data\n",
    "\n",
    "        store_dmri_data(scan_data_normalized, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data(mri_mudi_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data(mri_hcp_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Average bvecs and create scheme file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_bvecs(subjects: list[MRISubjectData]):\n",
    "    bvecs = None\n",
    "    other = None\n",
    "    for subject in subjects:\n",
    "        scheme = np.loadtxt(subject.scheme_path)\n",
    "        if bvecs is not None:\n",
    "            bvecs += scheme[:, :3]\n",
    "        else:\n",
    "            bvecs = scheme[:, :3]\n",
    "            other = scheme[:, 3:]\n",
    "\n",
    "    bvecs /= len(subjects)\n",
    "    for i in range(bvecs.shape[0]):\n",
    "        if other[i, 0] != 0.0:  # if b-value is not 0\n",
    "            bvecs[i] /= np.sqrt(np.sum(bvecs[i] ** 2))\n",
    "        else:\n",
    "            bvecs[i] = 0.0\n",
    "\n",
    "    return np.c_[bvecs, other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = average_bvecs(mri_mudi_subjects)\n",
    "np.savetxt(Path(MUDI_PATH, \"scheme.txt\"), scheme, fmt=\"%.6f %.6f %.6f %d %d %d\")\n",
    "with h5py.File(mri_mudi_subjects[0].output_file, \"a\") as hdf5_f:\n",
    "    hdf5_f.create_dataset(\"scheme\", data=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = average_bvecs(mri_hcp_subjects)\n",
    "np.savetxt(Path(HCP_PATH, \"scheme.txt\"), scheme, fmt=\"%.6f %.6f %.6f %d\")\n",
    "with h5py.File(mri_hcp_subjects[0].output_file, \"a\") as hdf5_f:\n",
    "    hdf5_f.create_dataset(\"scheme\", data=scheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate White matter, grey matter, etc, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_5tt_mask(subject: MRISubjectData):\n",
    "    \"\"\"Docs: https://mrtrix.readthedocs.io/en/latest/reference/commands/5ttgen.html#ttgen-fsl\"\"\"\n",
    "    logger.info(\"starting 5tt generation, this will take a few minutes...\")\n",
    "    output_path = Path(subject.root_path, \"5tt.nii\")\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"5ttgen\",\n",
    "            \"fsl\",\n",
    "            Path(subject.root_path, subject.t1_path),  # The input T1-weighted image\n",
    "            output_path,  # The output 5TT image\n",
    "            \"-mask\",\n",
    "            Path(subject.root_path, subject.brain_mask_path),\n",
    "            \"-force\",\n",
    "            \"-nocrop\",\n",
    "        ]\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        logger.error(\"could not finish generation, return code: %d\", result.returncode)\n",
    "        return\n",
    "\n",
    "    subject.fivett_mask_path = output_path\n",
    "    logger.info(\"finished 5tt generation, return code: %d\", result.returncode)\n",
    "    return result.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_and_mask_5tt(subject: MRISubjectData):\n",
    "    target_img = image.load_img(str(Path(subject.root_path, subject.brain_mask_path)))\n",
    "    target_affine = target_img.affine\n",
    "    target_shape = image.get_data(target_img).shape\n",
    "\n",
    "    source_img = image.load_img(str(subject.fivett_mask_path))\n",
    "    source_affine = source_img.affine\n",
    "    source_shape = image.get_data(source_img).shape\n",
    "\n",
    "    if not np.allclose(target_affine, source_affine):\n",
    "        source_img = image.resample_img(source_img, target_affine=target_affine, target_shape=target_shape)\n",
    "\n",
    "    source_img = image.binarize_img(source_img, threshold=1 / 5)\n",
    "\n",
    "    return np.transpose(apply_mask(source_img, target_img))\n",
    "\n",
    "\n",
    "def create_and_store_5tt(subjects):\n",
    "    for subject in subjects:\n",
    "        if subject.fivett_mask_path is None:\n",
    "            returncode = generate_5tt_mask(subject)\n",
    "            if returncode != 0:\n",
    "                logger.error(\"could not 5tt masks\")\n",
    "                continue\n",
    "\n",
    "        _5tt_data = binarize_and_mask_5tt(subject)\n",
    "\n",
    "        with h5py.File(subject.output_file, \"a\") as file:\n",
    "            if \"5tt\" not in file.keys():\n",
    "                file.create_dataset(\"5tt\", data=_5tt_data, chunks=True, maxshape=(None, _5tt_data.shape[1]))\n",
    "            else:\n",
    "                file[\"5tt\"].resize(file[\"5tt\"].shape[0] + _5tt_data.shape[0], axis=0)\n",
    "                file[\"5tt\"][-_5tt_data.shape[0] :] = _5tt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_store_5tt(mri_mudi_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MUDI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = image.index_img(str(Path(root_dir, \"cdmri0011\", img_file)), np.array([1, 10, 100, 1000]))\n",
    "\n",
    "for img in image.iter_img(selected_imgs):\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_anat(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each scan has a mask to mask the brain out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(str(Path(root_dir, \"cdmri0011\", msk_file)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = np.loadtxt(Path(root_dir, \"parameters_new.txt\"))\n",
    "\n",
    "\n",
    "def set_dir(r):\n",
    "    if r[3] == 0.0:\n",
    "        r[0:3] = 0.0\n",
    "    return r\n",
    "\n",
    "\n",
    "scheme = np.apply_along_axis(set_dir, 1, scheme)\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter3d(\n",
    "        x=scheme[:, 0],\n",
    "        y=scheme[:, 1],\n",
    "        z=scheme[:, 2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=scheme[:, 3], colorscale=\"Bluered\", showscale=True),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Q-space colored by b-values.\",\n",
    "    margin=dict(l=0, r=0, b=0),\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    scene=dict(\n",
    "        annotations=[\n",
    "            dict(\n",
    "                showarrow=False,\n",
    "                x=0,\n",
    "                y=0,\n",
    "                z=0,\n",
    "                text=\"b = 0\",\n",
    "                xanchor=\"left\",\n",
    "                xshift=10,\n",
    "                opacity=0.7,\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = list()  # cannot be numpy array as scans are irregular shaped.\n",
    "for name in [\"cdmri0011\", \"cdmri0012\", \"cdmri0013\", \"cdmri0014\", \"cdmri0015\"]:\n",
    "    scan_f_img = str(Path(root_dir, name, img_file))\n",
    "    scan_f_msk = str(Path(root_dir, name, msk_file))\n",
    "    scan = np.transpose(apply_mask(imgs=scan_f_img, mask_img=scan_f_msk))\n",
    "\n",
    "    scans.append(scan)\n",
    "\n",
    "    logger.debug(\"Loaded scan with shape: %s\", scan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "for i in range(5):\n",
    "    axes[i].hist(scans[i].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the 95th percentile of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = np.percentile(scans[0].flatten(), 95)\n",
    "logger.info(\"95th percentile max: %f\", max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "for i in range(5):\n",
    "    axes[i].hist(scans[i].flatten(), range=[0, max_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonize the data\n",
    "We want the different scans to have a similair distribution of values. We look at different techniques to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Median across all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_scans = list()\n",
    "for i in range(5):\n",
    "    median_scan = np.median(scans[i], axis=0)\n",
    "    median_scans.append(median_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_lstsq_coefs = np.empty((5, 2), dtype=np.float32)\n",
    "scan_lstsq_coefs[0] = np.asarray([1.0, 11])  # initialize with 1 because the coef of itself is 1\n",
    "\n",
    "for i in range(1, 5):\n",
    "    scan_lstsq_coef, _, _, _ = np.linalg.lstsq(median_scans[i][:, np.newaxis], median_scans[0], rcond=-1)\n",
    "    scan_lstsq_coefs[i] = np.asarray([scan_lstsq_coef[0], 11 + i])\n",
    "\n",
    "    logger.info(\"lstsq coefficient for scan %d: %f\", 11 + i, scan_lstsq_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 5))\n",
    "\n",
    "colors = [\"bo\", \"go\", \"ro\", \"co\", \"mo\"]\n",
    "for i in range(5):\n",
    "    axes[i].plot(\n",
    "        median_scans[i],\n",
    "        median_scans[0],\n",
    "        colors[i],\n",
    "        median_scans[i],\n",
    "        median_scans[i] * scan_lstsq_coefs[i, 0],\n",
    "        \":k\",\n",
    "        median_scans[i],\n",
    "        median_scans[i],\n",
    "        \"-k\",\n",
    "    )\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Median across b=0 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = np.loadtxt(Path(root_dir, \"parameters_new.txt\"))\n",
    "mask = scheme[:, 3] == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_scans = list()\n",
    "for i in range(5):\n",
    "    median_scan = np.median(scans[i][:, mask], axis=0)\n",
    "    median_scans.append(median_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_lstsq_coefs = np.empty((5, 2), dtype=np.float32)\n",
    "scan_lstsq_coefs[0] = np.asarray([1.0, 11])  # initialize with 1 because the coef of itself is 1\n",
    "\n",
    "for i in range(1, 5):\n",
    "    scan_lstsq_coef, _, _, _ = np.linalg.lstsq(median_scans[i][:, np.newaxis], median_scans[0], rcond=-1)\n",
    "    scan_lstsq_coefs[i] = np.asarray([scan_lstsq_coef[0], 11 + i])\n",
    "\n",
    "    logger.info(\"lstsq coefficient for scan %d: %f\", 11 + i, scan_lstsq_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 5))\n",
    "\n",
    "colors = [\"bo\", \"go\", \"ro\", \"co\", \"mo\"]\n",
    "for i in range(5):\n",
    "    axes[i].plot(\n",
    "        median_scans[i],\n",
    "        median_scans[0],\n",
    "        colors[i],\n",
    "        median_scans[i],\n",
    "        median_scans[i] * scan_lstsq_coefs[i, 0],\n",
    "        \":k\",\n",
    "        median_scans[i],\n",
    "        median_scans[i],\n",
    "        \"-k\",\n",
    "    )\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique seems to get us closer to scan 0, so we use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Normalize according to 99 percentile of Subject 11 and save in one big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data = masked_data11.max()\n",
    "max_data = np.percentile(masked_data11, 99)\n",
    "masked_data11n = masked_data11.astype(\"float32\") / max_data\n",
    "masked_data11n = np.clip(masked_data11n, 0, 1)\n",
    "masked_data12n = masked_data12.astype(\"float32\") * a12 / max_data\n",
    "masked_data12n = np.clip(masked_data12n, 0, 1)\n",
    "masked_data13n = masked_data13.astype(\"float32\") * a13 / max_data\n",
    "masked_data13n = np.clip(masked_data13n, 0, 1)\n",
    "masked_data14n = masked_data14.astype(\"float32\") * a14 / max_data\n",
    "masked_data14n = np.clip(masked_data14n, 0, 1)\n",
    "masked_data15n = masked_data15.astype(\"float32\") * a15 / max_data\n",
    "masked_data15n = np.clip(masked_data15n, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_data)\n",
    "print(masked_data11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(masked_data11, bins = 'auto')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj11 = 11 * np.ones((masked_data11.shape[0],), dtype=int)\n",
    "subj12 = 12 * np.ones((masked_data12.shape[0],), dtype=int)\n",
    "subj13 = 13 * np.ones((masked_data13.shape[0],), dtype=int)\n",
    "subj14 = 14 * np.ones((masked_data14.shape[0],), dtype=int)\n",
    "subj15 = 15 * np.ones((masked_data15.shape[0],), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.concatenate((subj11, subj12, subj13, subj14, subj15), axis=0)\n",
    "print(subj.shape)\n",
    "masked_data = np.concatenate(\n",
    "    (masked_data11n, masked_data12n, masked_data13n, masked_data14n, masked_data15n),\n",
    "    axis=0,\n",
    ")\n",
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate((subj[:, np.newaxis], masked_data), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Normalize according to 95 percentile of Subject 11 and save as separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data = masked_data11.max()\n",
    "max_data = np.percentile(masked_data11, 95)\n",
    "masked_data11n = masked_data11.astype(\"float32\") / max_data\n",
    "masked_data11n = np.clip(masked_data11n, 0, 1)\n",
    "masked_data12n = masked_data12.astype(\"float32\") * a12 / max_data\n",
    "masked_data12n = np.clip(masked_data12n, 0, 1)\n",
    "masked_data13n = masked_data13.astype(\"float32\") * a13 / max_data\n",
    "masked_data13n = np.clip(masked_data13n, 0, 1)\n",
    "masked_data14n = masked_data14.astype(\"float32\") * a14 / max_data\n",
    "masked_data14n = np.clip(masked_data14n, 0, 1)\n",
    "masked_data15n = masked_data15.astype(\"float32\") * a15 / max_data\n",
    "masked_data15n = np.clip(masked_data15n, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_data)\n",
    "print(masked_data11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "axes[0].hist(masked_data11n.flatten(), range=[0, 1])\n",
    "axes[1].hist(masked_data12n.flatten(), range=[0, 1])\n",
    "axes[2].hist(masked_data13n.flatten(), range=[0, 1])\n",
    "axes[3].hist(masked_data14n.flatten(), range=[0, 1])\n",
    "axes[4].hist(masked_data15n.flatten(), range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.DataFrame(np.concatenate((np.zeros_like(subj11[:, np.newaxis]), masked_data11n), axis=1))\n",
    "df12 = pd.DataFrame(np.concatenate((np.zeros_like(subj12[:, np.newaxis]), masked_data12n), axis=1))\n",
    "df13 = pd.DataFrame(np.concatenate((np.zeros_like(subj13[:, np.newaxis]), masked_data13n), axis=1))\n",
    "df14 = pd.DataFrame(np.concatenate((np.zeros_like(subj14[:, np.newaxis]), masked_data14n), axis=1))\n",
    "df15 = pd.DataFrame(np.concatenate((np.zeros_like(subj15[:, np.newaxis]), masked_data15n), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.to_csv(\"data11.csv\")\n",
    "df12.to_csv(\"data12.csv\")\n",
    "df13.to_csv(\"data13.csv\")\n",
    "df14.to_csv(\"data14.csv\")\n",
    "df15.to_csv(\"data15.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize according to 95 percentile of Subject 11, don't clip, and save in two big files (one 'header' and one 'data') (USE THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = np.percentile(scans[0], 95)\n",
    "logger.info(\"95th percentile: %f\", max_data)\n",
    "\n",
    "normalized_scans = list()\n",
    "for i in range(5):\n",
    "    normalized_scan = scans[i].astype(\"float32\") * scan_lstsq_coefs[i, 0] / max_data\n",
    "    normalized_scans.append(normalized_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].hist(normalized_scans[i].flatten(), range=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan data in one file. This makes it easier later for training. We save a seperate header file such that we can distinguish the different scans from each other later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(name: str, max_rows: Optional[int] = None):\n",
    "    \"\"\"Save the data to hdf5\n",
    "\n",
    "    Args:\n",
    "        name (str): name of the file.\n",
    "        max_rows (int, optional): use this to select a subset of the data. Used\n",
    "        for testing. Set to None to get all the data. Defaults to None.\n",
    "    \"\"\"\n",
    "    indexes = np.concatenate(\n",
    "        [(i + 11) * np.ones((scans[i].shape[0],), dtype=int)[:max_rows] for i in range(5)],\n",
    "        axis=0,\n",
    "    )\n",
    "    data = np.concatenate(\n",
    "        [normalized_scan[:max_rows, :] for normalized_scan in normalized_scans],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    hdf5_f_path = Path(root_dir, f\"{name}.hdf5\")\n",
    "    with h5py.File(hdf5_f_path, \"w\") as hdf5_f:\n",
    "        hdf5_f.create_dataset(\"data\", data=data)\n",
    "        hdf5_f.create_dataset(\"index\", data=indexes)\n",
    "        hdf5_f.create_dataset(\"scheme\", data=scheme)\n",
    "\n",
    "        normalization_data = hdf5_f.create_group(\"normalization_data\")\n",
    "        normalization_data.create_dataset(\"lstsq_coef\", data=scan_lstsq_coefs)\n",
    "        normalization_data.create_dataset(\"max_data\", data=np.asarray([max_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some fake data to test the correctness of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"data_fake\", max_rows=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed saving the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecf = \"/home/sapct5/Documents/Code/MUDI/MUDI_CA_LR/Run(lr=0.001, batch_size=100)K=500_epoch=800_testnone_unique.txt\"\n",
    "selecind = np.sort(np.loadtxt(selecf, dtype=int))\n",
    "print(selecind)\n",
    "\n",
    "mask = scheme[:, 3] == 0.0\n",
    "\n",
    "mask_ = np.zeros(np.shape(mask), dtype=bool)\n",
    "mask_[selecind] = True\n",
    "print(mask_.shape)\n",
    "\n",
    "mask3 = mask & mask_\n",
    "print(mask3.shape)\n",
    "\n",
    "scheme_ = scheme[mask_]\n",
    "print(scheme_.shape)\n",
    "\n",
    "mask2 = scheme_[:, 3] == 0.0\n",
    "print(mask2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc16 = \"./data\"\n",
    "masked_data16 = np.transpose(\n",
    "    apply_mask(\n",
    "        imgs=os.path.join(direc16, \"16_MB_RE_t.nii.gz\"),\n",
    "        mask_img=os.path.join(direc16, \"brain_mask-testing1.nii.gz\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc17 = \"./data\"\n",
    "masked_data17 = np.transpose(\n",
    "    apply_mask(\n",
    "        imgs=os.path.join(direc17, \"17_MB_RE_t.nii.gz\"),\n",
    "        mask_img=os.path.join(direc17, \"brain_mask-testing2.nii.gz\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med16 = np.median(masked_data16[:, mask2], axis=0)\n",
    "med17 = np.median(masked_data17[:, mask2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med11_ = np.median(masked_data11[:, mask3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a16, _, _, _ = np.linalg.lstsq(med16[:, np.newaxis], med11_)\n",
    "a17, _, _, _ = np.linalg.lstsq(med17[:, np.newaxis], med11_)\n",
    "print(a16, a17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(20, 5))\n",
    "axes[0].plot(med16, med11_, \"yo\", med16, med16 * a16, \":k\", med16, med16, \"-k\")\n",
    "axes[1].plot(med17, med11_, \"bo\", med17, med17 * a17, \":k\", med17, med17, \"-k\")\n",
    "for ax in axes:\n",
    "    ax.set(aspect=\"equal\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = image.index_img(os.path.join(direc11, img_file), np.array(selecind[[1, 10, 100, 300]]))\n",
    "for img in image.iter_img(selected_imgs):\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_anat(img, vmin=0, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = image.index_img(os.path.join(direc16, \"16_MB_RE_t.nii.gz\"), np.array([1, 10, 100, 300]))\n",
    "for img in image.iter_img(selected_imgs):\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_anat(img, vmin=0, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data16n = masked_data16.astype(\"float32\") * a16 / max_data\n",
    "masked_data17n = masked_data17.astype(\"float32\") * a17 / max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj16 = 16 * np.ones((masked_data16.shape[0],), dtype=int)\n",
    "subj17 = 17 * np.ones((masked_data17.shape[0],), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.concatenate((subj16, subj17), axis=0)\n",
    "print(subj.shape)\n",
    "masked_data = np.concatenate((masked_data16n, masked_data17n), axis=0)\n",
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(subj16) + len(subj17))\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.concatenate((ind[:, np.newaxis], subj[:, np.newaxis]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"header_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5f = h5py.File(\"data_test.hdf5\", \"w\")\n",
    "h5f.create_dataset(\"data1\", data=masked_data)\n",
    "h5f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# (not working yet) Log transform, normalise according to 95 percentile of Subject 11, don't clip, and save in two big files (one 'header' and one 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data = masked_data11.max()\n",
    "max_data = np.percentile(masked_data11, 95)\n",
    "masked_data11n = masked_data11.astype(\"float32\") / max_data\n",
    "masked_data12n = masked_data12.astype(\"float32\") * a12 / max_data\n",
    "masked_data13n = masked_data13.astype(\"float32\") * a13 / max_data\n",
    "masked_data14n = masked_data14.astype(\"float32\") * a14 / max_data\n",
    "masked_data15n = masked_data15.astype(\"float32\") * a15 / max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_data)\n",
    "print(masked_data11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "axes[0].hist(masked_data11n.flatten(), range=[0, 1])\n",
    "axes[1].hist(masked_data12n.flatten(), range=[0, 1])\n",
    "axes[2].hist(masked_data13n.flatten(), range=[0, 1])\n",
    "axes[3].hist(masked_data14n.flatten(), range=[0, 1])\n",
    "axes[4].hist(masked_data15n.flatten(), range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj11 = 11 * np.ones((masked_data11.shape[0],), dtype=int)\n",
    "subj12 = 12 * np.ones((masked_data12.shape[0],), dtype=int)\n",
    "subj13 = 13 * np.ones((masked_data13.shape[0],), dtype=int)\n",
    "subj14 = 14 * np.ones((masked_data14.shape[0],), dtype=int)\n",
    "subj15 = 15 * np.ones((masked_data15.shape[0],), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.concatenate((subj11, subj12, subj13, subj14, subj15), axis=0)\n",
    "print(subj.shape)\n",
    "masked_data = np.concatenate(\n",
    "    (masked_data11n, masked_data12n, masked_data13n, masked_data14n, masked_data15n),\n",
    "    axis=0,\n",
    ")\n",
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(subj11) + len(subj12) + len(subj13) + len(subj14) + len(subj15))\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.concatenate((ind[:, np.newaxis], subj[:, np.newaxis]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"header_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6623f10d21f7cd42380353d1b2afd90f10262effa2dc8ec2464c62b1bda5533"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
