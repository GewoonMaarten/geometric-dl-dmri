{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUDI data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from nilearn import image, plotting\n",
    "from nilearn.masking import apply_mask\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from autoencoder.logger import logger, set_log_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/media/maarten/disk1/MUDI/\"\n",
    "img_file = \"MB_Re_t_moco_registered_applytopup.nii.gz\"\n",
    "msk_file = \"brain_mask.nii.gz\"\n",
    "scheme_file = \"parameters_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MUDI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = image.index_img(\n",
    "    str(Path(root_dir, \"cdmri0011\", img_file)), np.array([1, 10, 100, 1000])\n",
    ")\n",
    "\n",
    "for img in image.iter_img(selected_imgs):\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_anat(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each scan has a mask to mask the brain out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(str(Path(root_dir, \"cdmri0011\", msk_file)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = np.loadtxt(Path(root_dir, \"parameters_new.txt\"))\n",
    "\n",
    "\n",
    "def set_dir(r):\n",
    "    if r[3] == 0.0:\n",
    "        r[0:3] = 0.0\n",
    "    return r\n",
    "\n",
    "\n",
    "scheme = np.apply_along_axis(set_dir, 1, scheme)\n",
    "fig = go.Figure(\n",
    "    data=go.Scatter3d(\n",
    "        x=scheme[:, 0],\n",
    "        y=scheme[:, 1],\n",
    "        z=scheme[:, 2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=scheme[:, 3], colorscale=\"Bluered\", showscale=True),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Q-space colored by b-values.\",\n",
    "    margin=dict(l=0, r=0, b=0),\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    scene=dict(\n",
    "        annotations=[\n",
    "            dict(\n",
    "                showarrow=False,\n",
    "                x=0,\n",
    "                y=0,\n",
    "                z=0,\n",
    "                text=\"b = 0\",\n",
    "                xanchor=\"left\",\n",
    "                xshift=10,\n",
    "                opacity=0.7,\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = list()  # cannot be numpy array as scans are irregular shaped.\n",
    "for name in [\"cdmri0011\", \"cdmri0012\", \"cdmri0013\", \"cdmri0014\", \"cdmri0015\"]:\n",
    "    scan_f_img = str(Path(root_dir, name, img_file))\n",
    "    scan_f_msk = str(Path(root_dir, name, msk_file))\n",
    "    scan = np.transpose(apply_mask(imgs=scan_f_img, mask_img=scan_f_msk))\n",
    "\n",
    "    scans.append(scan)\n",
    "\n",
    "    logger.debug(\"Loaded scan with shape: %s\", scan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "for i in range(5):\n",
    "    axes[i].hist(scans[i].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the 95th percentile of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = np.percentile(scans[0].flatten(), 95)\n",
    "logger.info(\"95th percentile max: %f\", max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "for i in range(5):\n",
    "    axes[i].hist(scans[i].flatten(), range=[0, max_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonize the data\n",
    "We want the different scans to have a similair distribution of values. We look at different techniques to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Median across all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_scans = list()\n",
    "for i in range(5):\n",
    "    median_scan = np.median(scans[i], axis=0)\n",
    "    median_scans.append(median_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_lstsq_coefs = np.empty((5, 2), dtype=np.float32)\n",
    "scan_lstsq_coefs[0] = np.asarray(\n",
    "    [1.0, 11]\n",
    ")  # initialize with 1 because the coef of itself is 1\n",
    "\n",
    "for i in range(1, 5):\n",
    "    scan_lstsq_coef, _, _, _ = np.linalg.lstsq(\n",
    "        median_scans[i][:, np.newaxis], median_scans[0], rcond=-1\n",
    "    )\n",
    "    scan_lstsq_coefs[i] = np.asarray([scan_lstsq_coef[0], 11 + i])\n",
    "\n",
    "    logger.info(\"lstsq coefficient for scan %d: %f\", 11 + i, scan_lstsq_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 5))\n",
    "\n",
    "colors = [\"bo\", \"go\", \"ro\", \"co\", \"mo\"]\n",
    "for i in range(5):\n",
    "    axes[i].plot(\n",
    "        median_scans[i],\n",
    "        median_scans[0],\n",
    "        colors[i],\n",
    "        median_scans[i],\n",
    "        median_scans[i] * scan_lstsq_coefs[i, 0],\n",
    "        \":k\",\n",
    "        median_scans[i],\n",
    "        median_scans[i],\n",
    "        \"-k\",\n",
    "    )\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Median across b=0 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = np.loadtxt(Path(root_dir, \"parameters_new.txt\"))\n",
    "mask = scheme[:, 3] == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_scans = list()\n",
    "for i in range(5):\n",
    "    median_scan = np.median(scans[i][:, mask], axis=0)\n",
    "    median_scans.append(median_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_lstsq_coefs = np.empty((5, 2), dtype=np.float32)\n",
    "scan_lstsq_coefs[0] = np.asarray(\n",
    "    [1.0, 11]\n",
    ")  # initialize with 1 because the coef of itself is 1\n",
    "\n",
    "for i in range(1, 5):\n",
    "    scan_lstsq_coef, _, _, _ = np.linalg.lstsq(\n",
    "        median_scans[i][:, np.newaxis], median_scans[0], rcond=-1\n",
    "    )\n",
    "    scan_lstsq_coefs[i] = np.asarray([scan_lstsq_coef[0], 11 + i])\n",
    "\n",
    "    logger.info(\"lstsq coefficient for scan %d: %f\", 11 + i, scan_lstsq_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 5))\n",
    "\n",
    "colors = [\"bo\", \"go\", \"ro\", \"co\", \"mo\"]\n",
    "for i in range(5):\n",
    "    axes[i].plot(\n",
    "        median_scans[i],\n",
    "        median_scans[0],\n",
    "        colors[i],\n",
    "        median_scans[i],\n",
    "        median_scans[i] * scan_lstsq_coefs[i, 0],\n",
    "        \":k\",\n",
    "        median_scans[i],\n",
    "        median_scans[i],\n",
    "        \"-k\",\n",
    "    )\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique seems to get us closer to scan 0, so we use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Normalize according to 99 percentile of Subject 11 and save in one big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data = masked_data11.max()\n",
    "max_data = np.percentile(masked_data11, 99)\n",
    "masked_data11n = masked_data11.astype(\"float32\") / max_data\n",
    "masked_data11n = np.clip(masked_data11n, 0, 1)\n",
    "masked_data12n = masked_data12.astype(\"float32\") * a12 / max_data\n",
    "masked_data12n = np.clip(masked_data12n, 0, 1)\n",
    "masked_data13n = masked_data13.astype(\"float32\") * a13 / max_data\n",
    "masked_data13n = np.clip(masked_data13n, 0, 1)\n",
    "masked_data14n = masked_data14.astype(\"float32\") * a14 / max_data\n",
    "masked_data14n = np.clip(masked_data14n, 0, 1)\n",
    "masked_data15n = masked_data15.astype(\"float32\") * a15 / max_data\n",
    "masked_data15n = np.clip(masked_data15n, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_data)\n",
    "print(masked_data11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(masked_data11, bins = 'auto')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj11 = 11 * np.ones((masked_data11.shape[0],), dtype=int)\n",
    "subj12 = 12 * np.ones((masked_data12.shape[0],), dtype=int)\n",
    "subj13 = 13 * np.ones((masked_data13.shape[0],), dtype=int)\n",
    "subj14 = 14 * np.ones((masked_data14.shape[0],), dtype=int)\n",
    "subj15 = 15 * np.ones((masked_data15.shape[0],), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.concatenate((subj11, subj12, subj13, subj14, subj15), axis=0)\n",
    "print(subj.shape)\n",
    "masked_data = np.concatenate(\n",
    "    (masked_data11n, masked_data12n, masked_data13n, masked_data14n, masked_data15n),\n",
    "    axis=0,\n",
    ")\n",
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate((subj[:, np.newaxis], masked_data), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Normalize according to 95 percentile of Subject 11 and save as separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data = masked_data11.max()\n",
    "max_data = np.percentile(masked_data11, 95)\n",
    "masked_data11n = masked_data11.astype(\"float32\") / max_data\n",
    "masked_data11n = np.clip(masked_data11n, 0, 1)\n",
    "masked_data12n = masked_data12.astype(\"float32\") * a12 / max_data\n",
    "masked_data12n = np.clip(masked_data12n, 0, 1)\n",
    "masked_data13n = masked_data13.astype(\"float32\") * a13 / max_data\n",
    "masked_data13n = np.clip(masked_data13n, 0, 1)\n",
    "masked_data14n = masked_data14.astype(\"float32\") * a14 / max_data\n",
    "masked_data14n = np.clip(masked_data14n, 0, 1)\n",
    "masked_data15n = masked_data15.astype(\"float32\") * a15 / max_data\n",
    "masked_data15n = np.clip(masked_data15n, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_data)\n",
    "print(masked_data11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "axes[0].hist(masked_data11n.flatten(), range=[0, 1])\n",
    "axes[1].hist(masked_data12n.flatten(), range=[0, 1])\n",
    "axes[2].hist(masked_data13n.flatten(), range=[0, 1])\n",
    "axes[3].hist(masked_data14n.flatten(), range=[0, 1])\n",
    "axes[4].hist(masked_data15n.flatten(), range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.DataFrame(\n",
    "    np.concatenate((np.zeros_like(subj11[:, np.newaxis]), masked_data11n), axis=1)\n",
    ")\n",
    "df12 = pd.DataFrame(\n",
    "    np.concatenate((np.zeros_like(subj12[:, np.newaxis]), masked_data12n), axis=1)\n",
    ")\n",
    "df13 = pd.DataFrame(\n",
    "    np.concatenate((np.zeros_like(subj13[:, np.newaxis]), masked_data13n), axis=1)\n",
    ")\n",
    "df14 = pd.DataFrame(\n",
    "    np.concatenate((np.zeros_like(subj14[:, np.newaxis]), masked_data14n), axis=1)\n",
    ")\n",
    "df15 = pd.DataFrame(\n",
    "    np.concatenate((np.zeros_like(subj15[:, np.newaxis]), masked_data15n), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.to_csv(\"data11.csv\")\n",
    "df12.to_csv(\"data12.csv\")\n",
    "df13.to_csv(\"data13.csv\")\n",
    "df14.to_csv(\"data14.csv\")\n",
    "df15.to_csv(\"data15.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize according to 95 percentile of Subject 11, don't clip, and save in two big files (one 'header' and one 'data') (USE THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = np.percentile(scans[0], 95)\n",
    "logger.info(\"95th percentile: %f\", max_data)\n",
    "\n",
    "normalized_scans = list()\n",
    "for i in range(5):\n",
    "    normalized_scan = scans[i].astype(\"float32\") * scan_lstsq_coefs[i, 0] / max_data\n",
    "    normalized_scans.append(normalized_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].hist(normalized_scans[i].flatten(), range=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scan data in one file. This makes it easier later for training. We save a seperate header file such that we can distinguish the different scans from each other later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(name: str, max_rows: Optional[int] = None):\n",
    "    \"\"\"Save the data to hdf5\n",
    "\n",
    "    Args:\n",
    "        name (str): name of the file.\n",
    "        max_rows (int, optional): use this to select a subset of the data. Used\n",
    "        for testing. Set to None to get all the data. Defaults to None.\n",
    "    \"\"\"\n",
    "    indexes = np.concatenate(\n",
    "        [\n",
    "            (i + 11) * np.ones((scans[i].shape[0],), dtype=int)[:max_rows]\n",
    "            for i in range(5)\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    data = np.concatenate(\n",
    "        [normalized_scan[:max_rows, :] for normalized_scan in normalized_scans],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    hdf5_f_path = Path(root_dir, f\"{name}.hdf5\")\n",
    "    with h5py.File(hdf5_f_path, \"w\") as hdf5_f:\n",
    "        hdf5_f.create_dataset(\"data\", data=data)\n",
    "        hdf5_f.create_dataset(\"index\", data=indexes)\n",
    "        hdf5_f.create_dataset(\"scheme\", data=scheme)\n",
    "\n",
    "        normalization_data = hdf5_f.create_group(\"normalization_data\")\n",
    "        normalization_data.create_dataset(\"lstsq_coef\", data=scan_lstsq_coefs)\n",
    "        normalization_data.create_dataset(\"max_data\", data=np.asarray([max_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some fake data to test the correctness of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"data_fake\", max_rows=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed saving the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecf = \"/home/sapct5/Documents/Code/MUDI/MUDI_CA_LR/Run(lr=0.001, batch_size=100)K=500_epoch=800_testnone_unique.txt\"\n",
    "selecind = np.sort(np.loadtxt(selecf, dtype=int))\n",
    "print(selecind)\n",
    "\n",
    "mask = scheme[:, 3] == 0.0\n",
    "\n",
    "mask_ = np.zeros(np.shape(mask), dtype=bool)\n",
    "mask_[selecind] = True\n",
    "print(mask_.shape)\n",
    "\n",
    "mask3 = mask & mask_\n",
    "print(mask3.shape)\n",
    "\n",
    "scheme_ = scheme[mask_]\n",
    "print(scheme_.shape)\n",
    "\n",
    "mask2 = scheme_[:, 3] == 0.0\n",
    "print(mask2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc16 = \"./data\"\n",
    "masked_data16 = np.transpose(\n",
    "    apply_mask(\n",
    "        imgs=os.path.join(direc16, \"16_MB_RE_t.nii.gz\"),\n",
    "        mask_img=os.path.join(direc16, \"brain_mask-testing1.nii.gz\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc17 = \"./data\"\n",
    "masked_data17 = np.transpose(\n",
    "    apply_mask(\n",
    "        imgs=os.path.join(direc17, \"17_MB_RE_t.nii.gz\"),\n",
    "        mask_img=os.path.join(direc17, \"brain_mask-testing2.nii.gz\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med16 = np.median(masked_data16[:, mask2], axis=0)\n",
    "med17 = np.median(masked_data17[:, mask2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med11_ = np.median(masked_data11[:, mask3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a16, _, _, _ = np.linalg.lstsq(med16[:, np.newaxis], med11_)\n",
    "a17, _, _, _ = np.linalg.lstsq(med17[:, np.newaxis], med11_)\n",
    "print(a16, a17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(20, 5))\n",
    "axes[0].plot(med16, med11_, \"yo\", med16, med16 * a16, \":k\", med16, med16, \"-k\")\n",
    "axes[1].plot(med17, med11_, \"bo\", med17, med17 * a17, \":k\", med17, med17, \"-k\")\n",
    "for ax in axes:\n",
    "    ax.set(aspect=\"equal\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = image.index_img(\n",
    "    os.path.join(direc11, img_file), np.array(selecind[[1, 10, 100, 300]])\n",
    ")\n",
    "for img in image.iter_img(selected_imgs):\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_anat(img, vmin=0, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_imgs = image.index_img(\n",
    "    os.path.join(direc16, \"16_MB_RE_t.nii.gz\"), np.array([1, 10, 100, 300])\n",
    ")\n",
    "for img in image.iter_img(selected_imgs):\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_anat(img, vmin=0, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data16n = masked_data16.astype(\"float32\") * a16 / max_data\n",
    "masked_data17n = masked_data17.astype(\"float32\") * a17 / max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj16 = 16 * np.ones((masked_data16.shape[0],), dtype=int)\n",
    "subj17 = 17 * np.ones((masked_data17.shape[0],), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.concatenate((subj16, subj17), axis=0)\n",
    "print(subj.shape)\n",
    "masked_data = np.concatenate((masked_data16n, masked_data17n), axis=0)\n",
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(subj16) + len(subj17))\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.concatenate((ind[:, np.newaxis], subj[:, np.newaxis]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"header_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5f = h5py.File(\"data_test.hdf5\", \"w\")\n",
    "h5f.create_dataset(\"data1\", data=masked_data)\n",
    "h5f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# (not working yet) Log transform, normalise according to 95 percentile of Subject 11, don't clip, and save in two big files (one 'header' and one 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data = masked_data11.max()\n",
    "max_data = np.percentile(masked_data11, 95)\n",
    "masked_data11n = masked_data11.astype(\"float32\") / max_data\n",
    "masked_data12n = masked_data12.astype(\"float32\") * a12 / max_data\n",
    "masked_data13n = masked_data13.astype(\"float32\") * a13 / max_data\n",
    "masked_data14n = masked_data14.astype(\"float32\") * a14 / max_data\n",
    "masked_data15n = masked_data15.astype(\"float32\") * a15 / max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_data)\n",
    "print(masked_data11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(20, 10))\n",
    "axes[0].hist(masked_data11n.flatten(), range=[0, 1])\n",
    "axes[1].hist(masked_data12n.flatten(), range=[0, 1])\n",
    "axes[2].hist(masked_data13n.flatten(), range=[0, 1])\n",
    "axes[3].hist(masked_data14n.flatten(), range=[0, 1])\n",
    "axes[4].hist(masked_data15n.flatten(), range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj11 = 11 * np.ones((masked_data11.shape[0],), dtype=int)\n",
    "subj12 = 12 * np.ones((masked_data12.shape[0],), dtype=int)\n",
    "subj13 = 13 * np.ones((masked_data13.shape[0],), dtype=int)\n",
    "subj14 = 14 * np.ones((masked_data14.shape[0],), dtype=int)\n",
    "subj15 = 15 * np.ones((masked_data15.shape[0],), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = np.concatenate((subj11, subj12, subj13, subj14, subj15), axis=0)\n",
    "print(subj.shape)\n",
    "masked_data = np.concatenate(\n",
    "    (masked_data11n, masked_data12n, masked_data13n, masked_data14n, masked_data15n),\n",
    "    axis=0,\n",
    ")\n",
    "print(masked_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(subj11) + len(subj12) + len(subj13) + len(subj14) + len(subj15))\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.concatenate((ind[:, np.newaxis], subj[:, np.newaxis]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"header_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6623f10d21f7cd42380353d1b2afd90f10262effa2dc8ec2464c62b1bda5533"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
